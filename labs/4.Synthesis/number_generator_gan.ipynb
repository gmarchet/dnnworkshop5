{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(100,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_shape=(self.latent_dim,)))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train.astype(np.float32) / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a half batch of new images\n",
    "            noise = np.random.normal(0, 1, (half_batch, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Sample generator input\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        #fig.suptitle(\"DCGAN: Generated digits\", fontsize=12)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 393,729\n",
      "Trainable params: 392,833\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 856,193\n",
      "Trainable params: 855,809\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 1.148359, acc.: 21.88%] [G loss: 0.990779]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 0.844449, acc.: 46.88%] [G loss: 0.914905]\n",
      "2 [D loss: 0.660298, acc.: 56.25%] [G loss: 0.823153]\n",
      "3 [D loss: 0.697182, acc.: 62.50%] [G loss: 1.441353]\n",
      "4 [D loss: 0.641947, acc.: 59.38%] [G loss: 1.397679]\n",
      "5 [D loss: 0.565258, acc.: 71.88%] [G loss: 1.211776]\n",
      "6 [D loss: 0.513829, acc.: 71.88%] [G loss: 0.933647]\n",
      "7 [D loss: 0.353127, acc.: 87.50%] [G loss: 1.159211]\n",
      "8 [D loss: 0.540959, acc.: 84.38%] [G loss: 1.163827]\n",
      "9 [D loss: 0.627752, acc.: 65.62%] [G loss: 0.989400]\n",
      "10 [D loss: 0.575173, acc.: 68.75%] [G loss: 0.916221]\n",
      "11 [D loss: 0.407147, acc.: 84.38%] [G loss: 0.952731]\n",
      "12 [D loss: 0.690662, acc.: 59.38%] [G loss: 0.994638]\n",
      "13 [D loss: 0.710530, acc.: 65.62%] [G loss: 0.804999]\n",
      "14 [D loss: 1.020631, acc.: 53.12%] [G loss: 0.687528]\n",
      "15 [D loss: 0.581943, acc.: 75.00%] [G loss: 0.755136]\n",
      "16 [D loss: 1.022227, acc.: 28.12%] [G loss: 0.925201]\n",
      "17 [D loss: 0.957538, acc.: 40.62%] [G loss: 1.116328]\n",
      "18 [D loss: 0.606571, acc.: 62.50%] [G loss: 1.440040]\n",
      "19 [D loss: 0.659177, acc.: 75.00%] [G loss: 1.401901]\n",
      "20 [D loss: 0.556602, acc.: 78.12%] [G loss: 1.589600]\n",
      "21 [D loss: 0.872383, acc.: 43.75%] [G loss: 1.808675]\n",
      "22 [D loss: 0.652507, acc.: 65.62%] [G loss: 1.468497]\n",
      "23 [D loss: 0.702844, acc.: 56.25%] [G loss: 1.733335]\n",
      "24 [D loss: 0.726156, acc.: 56.25%] [G loss: 1.699309]\n",
      "25 [D loss: 0.865456, acc.: 46.88%] [G loss: 1.330810]\n",
      "26 [D loss: 0.566647, acc.: 71.88%] [G loss: 1.115364]\n",
      "27 [D loss: 0.696054, acc.: 62.50%] [G loss: 1.067448]\n",
      "28 [D loss: 0.648027, acc.: 65.62%] [G loss: 0.906806]\n",
      "29 [D loss: 0.592525, acc.: 71.88%] [G loss: 0.753375]\n",
      "30 [D loss: 0.734105, acc.: 59.38%] [G loss: 0.977679]\n",
      "31 [D loss: 1.144708, acc.: 50.00%] [G loss: 1.479821]\n",
      "32 [D loss: 0.822422, acc.: 53.12%] [G loss: 1.307273]\n",
      "33 [D loss: 1.095845, acc.: 37.50%] [G loss: 1.409551]\n",
      "34 [D loss: 0.392341, acc.: 90.62%] [G loss: 1.460145]\n",
      "35 [D loss: 0.604566, acc.: 56.25%] [G loss: 1.010305]\n",
      "36 [D loss: 0.421040, acc.: 78.12%] [G loss: 0.855547]\n",
      "37 [D loss: 0.440571, acc.: 84.38%] [G loss: 1.111141]\n",
      "38 [D loss: 0.787361, acc.: 62.50%] [G loss: 1.038285]\n",
      "39 [D loss: 0.949920, acc.: 50.00%] [G loss: 1.328116]\n",
      "40 [D loss: 0.945599, acc.: 43.75%] [G loss: 1.374056]\n",
      "41 [D loss: 0.936988, acc.: 50.00%] [G loss: 1.178707]\n",
      "42 [D loss: 1.010083, acc.: 43.75%] [G loss: 0.851275]\n",
      "43 [D loss: 1.126704, acc.: 37.50%] [G loss: 0.779037]\n",
      "44 [D loss: 0.847575, acc.: 50.00%] [G loss: 0.834315]\n",
      "45 [D loss: 1.034623, acc.: 40.62%] [G loss: 1.095140]\n",
      "46 [D loss: 1.022865, acc.: 34.38%] [G loss: 1.346643]\n",
      "47 [D loss: 0.808819, acc.: 56.25%] [G loss: 1.399503]\n",
      "48 [D loss: 0.942215, acc.: 56.25%] [G loss: 1.050692]\n",
      "49 [D loss: 0.643655, acc.: 65.62%] [G loss: 0.611828]\n",
      "50 [D loss: 0.729577, acc.: 59.38%] [G loss: 0.864363]\n",
      "51 [D loss: 0.630864, acc.: 65.62%] [G loss: 0.793088]\n",
      "52 [D loss: 1.016798, acc.: 53.12%] [G loss: 1.032068]\n",
      "53 [D loss: 0.815445, acc.: 53.12%] [G loss: 1.292533]\n",
      "54 [D loss: 0.863729, acc.: 56.25%] [G loss: 1.376208]\n",
      "55 [D loss: 1.022399, acc.: 34.38%] [G loss: 1.070826]\n",
      "56 [D loss: 0.763390, acc.: 53.12%] [G loss: 0.669069]\n",
      "57 [D loss: 0.933116, acc.: 50.00%] [G loss: 1.282353]\n",
      "58 [D loss: 0.859954, acc.: 43.75%] [G loss: 1.291065]\n",
      "59 [D loss: 0.916069, acc.: 56.25%] [G loss: 1.160243]\n",
      "60 [D loss: 1.124482, acc.: 43.75%] [G loss: 1.174998]\n",
      "61 [D loss: 0.839708, acc.: 53.12%] [G loss: 1.142682]\n",
      "62 [D loss: 0.618387, acc.: 65.62%] [G loss: 1.089308]\n",
      "63 [D loss: 0.993071, acc.: 40.62%] [G loss: 1.406584]\n",
      "64 [D loss: 1.006108, acc.: 43.75%] [G loss: 1.336562]\n",
      "65 [D loss: 0.800286, acc.: 62.50%] [G loss: 1.140016]\n",
      "66 [D loss: 1.151551, acc.: 31.25%] [G loss: 1.395230]\n",
      "67 [D loss: 0.876245, acc.: 40.62%] [G loss: 1.328830]\n",
      "68 [D loss: 0.764673, acc.: 43.75%] [G loss: 1.208844]\n",
      "69 [D loss: 0.899167, acc.: 43.75%] [G loss: 1.582906]\n",
      "70 [D loss: 0.739317, acc.: 50.00%] [G loss: 1.382488]\n",
      "71 [D loss: 1.079574, acc.: 34.38%] [G loss: 1.543686]\n",
      "72 [D loss: 0.893223, acc.: 50.00%] [G loss: 0.951499]\n",
      "73 [D loss: 0.931772, acc.: 43.75%] [G loss: 1.139770]\n",
      "74 [D loss: 0.822040, acc.: 53.12%] [G loss: 1.264181]\n",
      "75 [D loss: 0.879463, acc.: 50.00%] [G loss: 1.270214]\n",
      "76 [D loss: 1.114761, acc.: 40.62%] [G loss: 1.346409]\n",
      "77 [D loss: 0.917148, acc.: 46.88%] [G loss: 1.166024]\n",
      "78 [D loss: 0.917839, acc.: 50.00%] [G loss: 0.882788]\n",
      "79 [D loss: 1.132579, acc.: 37.50%] [G loss: 0.921907]\n",
      "80 [D loss: 1.066756, acc.: 43.75%] [G loss: 1.426426]\n",
      "81 [D loss: 0.558433, acc.: 78.12%] [G loss: 1.332189]\n",
      "82 [D loss: 0.984436, acc.: 43.75%] [G loss: 1.033653]\n",
      "83 [D loss: 0.925457, acc.: 37.50%] [G loss: 0.937223]\n",
      "84 [D loss: 0.875298, acc.: 56.25%] [G loss: 1.068758]\n",
      "85 [D loss: 0.848173, acc.: 59.38%] [G loss: 0.855471]\n",
      "86 [D loss: 0.991934, acc.: 40.62%] [G loss: 0.948821]\n",
      "87 [D loss: 0.751085, acc.: 62.50%] [G loss: 1.104233]\n",
      "88 [D loss: 0.843337, acc.: 46.88%] [G loss: 1.342044]\n",
      "89 [D loss: 0.805398, acc.: 56.25%] [G loss: 1.137684]\n",
      "90 [D loss: 0.994506, acc.: 40.62%] [G loss: 1.140770]\n",
      "91 [D loss: 1.042440, acc.: 46.88%] [G loss: 0.828277]\n",
      "92 [D loss: 0.876980, acc.: 43.75%] [G loss: 1.313009]\n",
      "93 [D loss: 0.831419, acc.: 50.00%] [G loss: 1.249427]\n",
      "94 [D loss: 0.954416, acc.: 43.75%] [G loss: 1.531014]\n",
      "95 [D loss: 1.023425, acc.: 37.50%] [G loss: 1.021665]\n",
      "96 [D loss: 1.289429, acc.: 37.50%] [G loss: 0.728702]\n",
      "97 [D loss: 1.074217, acc.: 34.38%] [G loss: 1.303063]\n",
      "98 [D loss: 0.941602, acc.: 46.88%] [G loss: 1.318012]\n",
      "99 [D loss: 0.944841, acc.: 46.88%] [G loss: 0.994531]\n",
      "100 [D loss: 0.852731, acc.: 50.00%] [G loss: 0.980408]\n",
      "101 [D loss: 0.993465, acc.: 46.88%] [G loss: 1.102984]\n",
      "102 [D loss: 0.985120, acc.: 40.62%] [G loss: 1.262523]\n",
      "103 [D loss: 1.076917, acc.: 37.50%] [G loss: 1.220660]\n",
      "104 [D loss: 1.004260, acc.: 28.12%] [G loss: 1.158718]\n",
      "105 [D loss: 1.137806, acc.: 28.12%] [G loss: 0.941266]\n",
      "106 [D loss: 0.879268, acc.: 46.88%] [G loss: 1.036682]\n",
      "107 [D loss: 1.048433, acc.: 28.12%] [G loss: 1.255657]\n",
      "108 [D loss: 0.918843, acc.: 53.12%] [G loss: 1.162613]\n",
      "109 [D loss: 0.873422, acc.: 40.62%] [G loss: 0.976798]\n",
      "110 [D loss: 0.810185, acc.: 56.25%] [G loss: 0.983320]\n",
      "111 [D loss: 0.831199, acc.: 53.12%] [G loss: 0.855218]\n",
      "112 [D loss: 1.302647, acc.: 31.25%] [G loss: 0.814300]\n",
      "113 [D loss: 0.777236, acc.: 56.25%] [G loss: 1.398340]\n",
      "114 [D loss: 1.001518, acc.: 40.62%] [G loss: 1.205067]\n",
      "115 [D loss: 0.991123, acc.: 31.25%] [G loss: 1.110358]\n",
      "116 [D loss: 0.866670, acc.: 46.88%] [G loss: 0.934782]\n",
      "117 [D loss: 0.943485, acc.: 43.75%] [G loss: 0.832690]\n",
      "118 [D loss: 0.819816, acc.: 46.88%] [G loss: 1.109243]\n",
      "119 [D loss: 1.119075, acc.: 34.38%] [G loss: 1.212147]\n",
      "120 [D loss: 0.750320, acc.: 59.38%] [G loss: 1.133868]\n",
      "121 [D loss: 0.827085, acc.: 53.12%] [G loss: 0.991988]\n",
      "122 [D loss: 1.088925, acc.: 40.62%] [G loss: 0.953827]\n",
      "123 [D loss: 0.790039, acc.: 59.38%] [G loss: 0.824910]\n",
      "124 [D loss: 0.889800, acc.: 46.88%] [G loss: 1.126269]\n",
      "125 [D loss: 1.054927, acc.: 40.62%] [G loss: 1.295805]\n",
      "126 [D loss: 0.724580, acc.: 53.12%] [G loss: 0.915770]\n",
      "127 [D loss: 0.976295, acc.: 53.12%] [G loss: 1.077694]\n",
      "128 [D loss: 0.928903, acc.: 43.75%] [G loss: 1.160771]\n",
      "129 [D loss: 0.823366, acc.: 56.25%] [G loss: 1.003863]\n",
      "130 [D loss: 0.904122, acc.: 46.88%] [G loss: 1.122252]\n",
      "131 [D loss: 1.020920, acc.: 40.62%] [G loss: 1.213639]\n",
      "132 [D loss: 1.028808, acc.: 43.75%] [G loss: 1.276953]\n",
      "133 [D loss: 1.040611, acc.: 31.25%] [G loss: 1.198756]\n",
      "134 [D loss: 0.760799, acc.: 50.00%] [G loss: 0.754015]\n",
      "135 [D loss: 0.890441, acc.: 46.88%] [G loss: 1.024328]\n",
      "136 [D loss: 0.781992, acc.: 46.88%] [G loss: 1.098963]\n",
      "137 [D loss: 0.851741, acc.: 56.25%] [G loss: 1.042442]\n",
      "138 [D loss: 0.900407, acc.: 50.00%] [G loss: 1.111738]\n",
      "139 [D loss: 0.774487, acc.: 59.38%] [G loss: 1.349228]\n",
      "140 [D loss: 0.910475, acc.: 43.75%] [G loss: 1.299581]\n",
      "141 [D loss: 0.930709, acc.: 50.00%] [G loss: 1.187249]\n",
      "142 [D loss: 1.034953, acc.: 37.50%] [G loss: 1.117249]\n",
      "143 [D loss: 0.661297, acc.: 59.38%] [G loss: 0.852228]\n",
      "144 [D loss: 0.852015, acc.: 37.50%] [G loss: 1.043955]\n",
      "145 [D loss: 0.870342, acc.: 50.00%] [G loss: 0.940179]\n",
      "146 [D loss: 1.015740, acc.: 37.50%] [G loss: 0.981296]\n",
      "147 [D loss: 0.769258, acc.: 59.38%] [G loss: 1.134421]\n",
      "148 [D loss: 0.887238, acc.: 37.50%] [G loss: 0.934018]\n",
      "149 [D loss: 0.744624, acc.: 50.00%] [G loss: 0.866130]\n",
      "150 [D loss: 0.842473, acc.: 50.00%] [G loss: 0.912070]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 [D loss: 0.752723, acc.: 56.25%] [G loss: 1.076088]\n",
      "152 [D loss: 0.833663, acc.: 56.25%] [G loss: 1.379928]\n",
      "153 [D loss: 0.763785, acc.: 59.38%] [G loss: 1.216852]\n",
      "154 [D loss: 0.769362, acc.: 59.38%] [G loss: 0.949765]\n",
      "155 [D loss: 0.969296, acc.: 40.62%] [G loss: 0.724516]\n",
      "156 [D loss: 0.929952, acc.: 43.75%] [G loss: 0.915444]\n",
      "157 [D loss: 0.791298, acc.: 53.12%] [G loss: 1.240051]\n",
      "158 [D loss: 0.731949, acc.: 53.12%] [G loss: 1.220877]\n",
      "159 [D loss: 0.978541, acc.: 34.38%] [G loss: 0.942580]\n",
      "160 [D loss: 0.883829, acc.: 40.62%] [G loss: 0.945182]\n",
      "161 [D loss: 0.927619, acc.: 50.00%] [G loss: 0.718518]\n",
      "162 [D loss: 0.997613, acc.: 50.00%] [G loss: 1.040340]\n",
      "163 [D loss: 0.898997, acc.: 50.00%] [G loss: 1.068262]\n",
      "164 [D loss: 0.880921, acc.: 43.75%] [G loss: 1.013907]\n",
      "165 [D loss: 0.828575, acc.: 56.25%] [G loss: 0.847318]\n",
      "166 [D loss: 0.975275, acc.: 43.75%] [G loss: 0.864780]\n",
      "167 [D loss: 0.826729, acc.: 46.88%] [G loss: 0.866853]\n",
      "168 [D loss: 0.862551, acc.: 50.00%] [G loss: 0.876600]\n",
      "169 [D loss: 1.039776, acc.: 28.12%] [G loss: 0.911222]\n",
      "170 [D loss: 0.918351, acc.: 53.12%] [G loss: 0.980996]\n",
      "171 [D loss: 1.088690, acc.: 34.38%] [G loss: 1.041741]\n",
      "172 [D loss: 0.898881, acc.: 43.75%] [G loss: 1.315231]\n",
      "173 [D loss: 0.859031, acc.: 43.75%] [G loss: 1.244300]\n",
      "174 [D loss: 0.866289, acc.: 53.12%] [G loss: 1.033327]\n",
      "175 [D loss: 0.821239, acc.: 50.00%] [G loss: 0.981793]\n",
      "176 [D loss: 0.910446, acc.: 50.00%] [G loss: 1.092371]\n",
      "177 [D loss: 0.886534, acc.: 43.75%] [G loss: 1.112701]\n",
      "178 [D loss: 0.753677, acc.: 53.12%] [G loss: 1.247015]\n",
      "179 [D loss: 0.974721, acc.: 31.25%] [G loss: 1.040094]\n",
      "180 [D loss: 0.815039, acc.: 56.25%] [G loss: 1.140231]\n",
      "181 [D loss: 0.826389, acc.: 50.00%] [G loss: 1.096921]\n",
      "182 [D loss: 1.037144, acc.: 37.50%] [G loss: 1.100096]\n",
      "183 [D loss: 1.029896, acc.: 40.62%] [G loss: 1.073033]\n",
      "184 [D loss: 0.727261, acc.: 56.25%] [G loss: 1.117299]\n",
      "185 [D loss: 0.675485, acc.: 65.62%] [G loss: 1.208870]\n",
      "186 [D loss: 0.706587, acc.: 56.25%] [G loss: 0.838992]\n",
      "187 [D loss: 0.731594, acc.: 56.25%] [G loss: 0.649300]\n",
      "188 [D loss: 0.804457, acc.: 56.25%] [G loss: 0.694805]\n",
      "189 [D loss: 0.743110, acc.: 53.12%] [G loss: 0.842271]\n",
      "190 [D loss: 0.916285, acc.: 46.88%] [G loss: 1.009130]\n",
      "191 [D loss: 0.725213, acc.: 59.38%] [G loss: 1.066238]\n",
      "192 [D loss: 0.821338, acc.: 43.75%] [G loss: 0.927370]\n",
      "193 [D loss: 1.091545, acc.: 34.38%] [G loss: 0.796322]\n",
      "194 [D loss: 0.909599, acc.: 34.38%] [G loss: 0.936942]\n",
      "195 [D loss: 0.753817, acc.: 59.38%] [G loss: 0.837440]\n",
      "196 [D loss: 0.800039, acc.: 56.25%] [G loss: 1.017163]\n",
      "197 [D loss: 0.904468, acc.: 43.75%] [G loss: 0.983152]\n",
      "198 [D loss: 0.946484, acc.: 31.25%] [G loss: 1.195576]\n",
      "199 [D loss: 0.795534, acc.: 53.12%] [G loss: 1.185249]\n",
      "200 [D loss: 0.898229, acc.: 43.75%] [G loss: 1.083896]\n",
      "201 [D loss: 0.954138, acc.: 34.38%] [G loss: 1.121880]\n",
      "202 [D loss: 0.893557, acc.: 40.62%] [G loss: 1.073325]\n",
      "203 [D loss: 0.948169, acc.: 53.12%] [G loss: 1.071914]\n",
      "204 [D loss: 0.961816, acc.: 31.25%] [G loss: 1.100634]\n",
      "205 [D loss: 0.892039, acc.: 50.00%] [G loss: 0.930685]\n",
      "206 [D loss: 0.959007, acc.: 43.75%] [G loss: 0.897379]\n",
      "207 [D loss: 0.748813, acc.: 53.12%] [G loss: 1.118274]\n",
      "208 [D loss: 0.927446, acc.: 59.38%] [G loss: 1.180011]\n",
      "209 [D loss: 0.942709, acc.: 46.88%] [G loss: 1.026385]\n",
      "210 [D loss: 0.795553, acc.: 50.00%] [G loss: 1.190229]\n",
      "211 [D loss: 0.901481, acc.: 31.25%] [G loss: 0.940845]\n",
      "212 [D loss: 1.025057, acc.: 37.50%] [G loss: 0.916682]\n",
      "213 [D loss: 0.866909, acc.: 43.75%] [G loss: 0.754154]\n",
      "214 [D loss: 0.874753, acc.: 40.62%] [G loss: 1.000909]\n",
      "215 [D loss: 0.880568, acc.: 43.75%] [G loss: 0.885251]\n",
      "216 [D loss: 0.942297, acc.: 34.38%] [G loss: 1.067665]\n",
      "217 [D loss: 0.792989, acc.: 59.38%] [G loss: 0.877605]\n",
      "218 [D loss: 0.840650, acc.: 34.38%] [G loss: 0.990587]\n",
      "219 [D loss: 1.034252, acc.: 37.50%] [G loss: 1.011750]\n",
      "220 [D loss: 0.869564, acc.: 53.12%] [G loss: 0.908727]\n",
      "221 [D loss: 0.697451, acc.: 56.25%] [G loss: 0.870130]\n",
      "222 [D loss: 0.899504, acc.: 40.62%] [G loss: 1.331656]\n",
      "223 [D loss: 1.093558, acc.: 31.25%] [G loss: 1.100538]\n",
      "224 [D loss: 0.745711, acc.: 62.50%] [G loss: 1.153144]\n",
      "225 [D loss: 1.068439, acc.: 31.25%] [G loss: 0.895804]\n",
      "226 [D loss: 0.996266, acc.: 25.00%] [G loss: 1.028859]\n",
      "227 [D loss: 0.802087, acc.: 59.38%] [G loss: 1.019120]\n",
      "228 [D loss: 1.143267, acc.: 34.38%] [G loss: 1.164220]\n",
      "229 [D loss: 0.853320, acc.: 50.00%] [G loss: 0.952942]\n",
      "230 [D loss: 0.954549, acc.: 46.88%] [G loss: 1.039773]\n",
      "231 [D loss: 0.816515, acc.: 46.88%] [G loss: 1.108371]\n",
      "232 [D loss: 0.835613, acc.: 53.12%] [G loss: 1.181211]\n",
      "233 [D loss: 0.808896, acc.: 56.25%] [G loss: 1.070111]\n",
      "234 [D loss: 0.877049, acc.: 50.00%] [G loss: 1.092047]\n",
      "235 [D loss: 0.779939, acc.: 53.12%] [G loss: 1.207832]\n",
      "236 [D loss: 0.784245, acc.: 46.88%] [G loss: 0.919943]\n",
      "237 [D loss: 0.952919, acc.: 28.12%] [G loss: 0.986265]\n",
      "238 [D loss: 0.801344, acc.: 43.75%] [G loss: 1.141881]\n",
      "239 [D loss: 0.824381, acc.: 46.88%] [G loss: 1.074022]\n",
      "240 [D loss: 0.987777, acc.: 31.25%] [G loss: 0.927515]\n",
      "241 [D loss: 0.953264, acc.: 40.62%] [G loss: 0.967526]\n",
      "242 [D loss: 0.733179, acc.: 50.00%] [G loss: 1.102019]\n",
      "243 [D loss: 0.879996, acc.: 40.62%] [G loss: 1.052913]\n",
      "244 [D loss: 0.883108, acc.: 43.75%] [G loss: 0.901976]\n",
      "245 [D loss: 0.813048, acc.: 62.50%] [G loss: 1.026775]\n",
      "246 [D loss: 0.911769, acc.: 40.62%] [G loss: 1.045950]\n",
      "247 [D loss: 0.920982, acc.: 40.62%] [G loss: 0.983068]\n",
      "248 [D loss: 0.871676, acc.: 43.75%] [G loss: 1.339267]\n",
      "249 [D loss: 0.848208, acc.: 53.12%] [G loss: 1.052478]\n",
      "250 [D loss: 0.849324, acc.: 46.88%] [G loss: 1.117549]\n",
      "251 [D loss: 0.991720, acc.: 37.50%] [G loss: 1.009688]\n",
      "252 [D loss: 0.711607, acc.: 56.25%] [G loss: 0.910365]\n",
      "253 [D loss: 0.991358, acc.: 31.25%] [G loss: 1.062233]\n",
      "254 [D loss: 0.996420, acc.: 37.50%] [G loss: 1.042039]\n",
      "255 [D loss: 0.908949, acc.: 37.50%] [G loss: 0.975451]\n",
      "256 [D loss: 0.976882, acc.: 31.25%] [G loss: 0.966106]\n",
      "257 [D loss: 0.780010, acc.: 56.25%] [G loss: 0.951931]\n",
      "258 [D loss: 0.640735, acc.: 68.75%] [G loss: 1.078719]\n",
      "259 [D loss: 0.775209, acc.: 56.25%] [G loss: 0.902143]\n",
      "260 [D loss: 0.829246, acc.: 56.25%] [G loss: 0.766338]\n",
      "261 [D loss: 0.920604, acc.: 34.38%] [G loss: 0.975487]\n",
      "262 [D loss: 0.698766, acc.: 53.12%] [G loss: 0.979416]\n",
      "263 [D loss: 0.824980, acc.: 53.12%] [G loss: 0.923728]\n",
      "264 [D loss: 1.008334, acc.: 28.12%] [G loss: 0.820727]\n",
      "265 [D loss: 0.826433, acc.: 46.88%] [G loss: 0.834865]\n",
      "266 [D loss: 1.077704, acc.: 40.62%] [G loss: 0.925624]\n",
      "267 [D loss: 0.708759, acc.: 53.12%] [G loss: 1.241785]\n",
      "268 [D loss: 0.870759, acc.: 53.12%] [G loss: 0.676377]\n",
      "269 [D loss: 1.039191, acc.: 31.25%] [G loss: 0.818599]\n",
      "270 [D loss: 0.904399, acc.: 34.38%] [G loss: 0.754285]\n",
      "271 [D loss: 0.908682, acc.: 46.88%] [G loss: 0.853608]\n",
      "272 [D loss: 0.838185, acc.: 53.12%] [G loss: 0.994528]\n",
      "273 [D loss: 0.881350, acc.: 53.12%] [G loss: 1.042586]\n",
      "274 [D loss: 0.649276, acc.: 65.62%] [G loss: 0.948024]\n",
      "275 [D loss: 1.019620, acc.: 31.25%] [G loss: 1.023661]\n",
      "276 [D loss: 0.798970, acc.: 46.88%] [G loss: 0.900134]\n",
      "277 [D loss: 0.822030, acc.: 34.38%] [G loss: 1.118039]\n",
      "278 [D loss: 0.914815, acc.: 43.75%] [G loss: 1.020609]\n",
      "279 [D loss: 0.651056, acc.: 62.50%] [G loss: 1.103922]\n",
      "280 [D loss: 0.758016, acc.: 56.25%] [G loss: 0.903076]\n",
      "281 [D loss: 0.830710, acc.: 46.88%] [G loss: 0.712090]\n",
      "282 [D loss: 0.730962, acc.: 59.38%] [G loss: 1.136150]\n",
      "283 [D loss: 0.711502, acc.: 50.00%] [G loss: 1.035244]\n",
      "284 [D loss: 0.831918, acc.: 37.50%] [G loss: 0.938565]\n",
      "285 [D loss: 0.827537, acc.: 37.50%] [G loss: 0.802054]\n",
      "286 [D loss: 0.923944, acc.: 40.62%] [G loss: 0.880191]\n",
      "287 [D loss: 0.917660, acc.: 43.75%] [G loss: 0.713957]\n",
      "288 [D loss: 1.001541, acc.: 28.12%] [G loss: 0.896850]\n",
      "289 [D loss: 0.903617, acc.: 40.62%] [G loss: 1.100754]\n",
      "290 [D loss: 0.792916, acc.: 50.00%] [G loss: 1.196278]\n",
      "291 [D loss: 0.931734, acc.: 43.75%] [G loss: 1.085381]\n",
      "292 [D loss: 0.778365, acc.: 46.88%] [G loss: 1.205790]\n",
      "293 [D loss: 0.897011, acc.: 56.25%] [G loss: 0.970390]\n",
      "294 [D loss: 1.151481, acc.: 31.25%] [G loss: 0.859405]\n",
      "295 [D loss: 0.791864, acc.: 53.12%] [G loss: 0.940415]\n",
      "296 [D loss: 0.762535, acc.: 53.12%] [G loss: 0.975705]\n",
      "297 [D loss: 0.931835, acc.: 40.62%] [G loss: 1.057344]\n",
      "298 [D loss: 0.905851, acc.: 34.38%] [G loss: 1.198523]\n",
      "299 [D loss: 0.836463, acc.: 56.25%] [G loss: 0.800926]\n",
      "300 [D loss: 0.770362, acc.: 53.12%] [G loss: 0.854309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301 [D loss: 0.920223, acc.: 37.50%] [G loss: 1.046093]\n",
      "302 [D loss: 0.909049, acc.: 34.38%] [G loss: 1.202247]\n",
      "303 [D loss: 1.012792, acc.: 34.38%] [G loss: 0.932703]\n",
      "304 [D loss: 0.791550, acc.: 46.88%] [G loss: 1.066933]\n",
      "305 [D loss: 0.914955, acc.: 50.00%] [G loss: 0.908118]\n",
      "306 [D loss: 0.786333, acc.: 53.12%] [G loss: 1.022485]\n",
      "307 [D loss: 0.780288, acc.: 50.00%] [G loss: 1.056467]\n",
      "308 [D loss: 0.838594, acc.: 50.00%] [G loss: 0.803846]\n",
      "309 [D loss: 0.798588, acc.: 50.00%] [G loss: 0.782498]\n",
      "310 [D loss: 0.880372, acc.: 46.88%] [G loss: 0.950231]\n",
      "311 [D loss: 0.763916, acc.: 56.25%] [G loss: 0.696178]\n",
      "312 [D loss: 0.728215, acc.: 56.25%] [G loss: 1.044043]\n",
      "313 [D loss: 0.742991, acc.: 46.88%] [G loss: 1.124492]\n",
      "314 [D loss: 0.784671, acc.: 53.12%] [G loss: 0.992454]\n",
      "315 [D loss: 0.819511, acc.: 50.00%] [G loss: 0.905306]\n",
      "316 [D loss: 0.828331, acc.: 50.00%] [G loss: 0.704213]\n",
      "317 [D loss: 0.861271, acc.: 43.75%] [G loss: 0.998016]\n",
      "318 [D loss: 1.033657, acc.: 25.00%] [G loss: 1.057461]\n",
      "319 [D loss: 0.891597, acc.: 40.62%] [G loss: 0.973910]\n",
      "320 [D loss: 0.898074, acc.: 37.50%] [G loss: 0.934281]\n",
      "321 [D loss: 0.917270, acc.: 40.62%] [G loss: 1.196290]\n",
      "322 [D loss: 0.784801, acc.: 53.12%] [G loss: 1.075958]\n",
      "323 [D loss: 0.777787, acc.: 50.00%] [G loss: 1.019866]\n",
      "324 [D loss: 0.898832, acc.: 43.75%] [G loss: 1.201593]\n",
      "325 [D loss: 0.925901, acc.: 37.50%] [G loss: 0.949894]\n",
      "326 [D loss: 0.808440, acc.: 50.00%] [G loss: 1.056132]\n",
      "327 [D loss: 0.704830, acc.: 68.75%] [G loss: 1.120843]\n",
      "328 [D loss: 0.758139, acc.: 53.12%] [G loss: 0.950988]\n",
      "329 [D loss: 0.853889, acc.: 37.50%] [G loss: 0.890764]\n",
      "330 [D loss: 1.085601, acc.: 31.25%] [G loss: 0.832713]\n",
      "331 [D loss: 1.068044, acc.: 37.50%] [G loss: 0.728815]\n",
      "332 [D loss: 0.926491, acc.: 28.12%] [G loss: 1.007003]\n",
      "333 [D loss: 0.821510, acc.: 50.00%] [G loss: 0.848698]\n",
      "334 [D loss: 0.936341, acc.: 34.38%] [G loss: 1.022341]\n",
      "335 [D loss: 0.938940, acc.: 37.50%] [G loss: 0.868888]\n",
      "336 [D loss: 0.757485, acc.: 53.12%] [G loss: 0.972605]\n",
      "337 [D loss: 0.870372, acc.: 43.75%] [G loss: 0.871893]\n",
      "338 [D loss: 0.748758, acc.: 59.38%] [G loss: 0.924778]\n",
      "339 [D loss: 0.884190, acc.: 46.88%] [G loss: 0.968302]\n",
      "340 [D loss: 0.925508, acc.: 46.88%] [G loss: 1.083498]\n",
      "341 [D loss: 0.903957, acc.: 40.62%] [G loss: 1.049789]\n",
      "342 [D loss: 0.897012, acc.: 53.12%] [G loss: 1.196849]\n",
      "343 [D loss: 0.789619, acc.: 46.88%] [G loss: 0.991978]\n",
      "344 [D loss: 0.841749, acc.: 56.25%] [G loss: 0.961132]\n",
      "345 [D loss: 0.807901, acc.: 53.12%] [G loss: 1.046265]\n",
      "346 [D loss: 0.742656, acc.: 50.00%] [G loss: 1.111088]\n",
      "347 [D loss: 0.921912, acc.: 40.62%] [G loss: 0.992289]\n",
      "348 [D loss: 0.939654, acc.: 46.88%] [G loss: 1.139570]\n",
      "349 [D loss: 0.739467, acc.: 53.12%] [G loss: 1.019697]\n",
      "350 [D loss: 0.651242, acc.: 65.62%] [G loss: 1.036613]\n",
      "351 [D loss: 0.741851, acc.: 53.12%] [G loss: 0.947512]\n",
      "352 [D loss: 0.895766, acc.: 37.50%] [G loss: 0.916672]\n",
      "353 [D loss: 0.672786, acc.: 65.62%] [G loss: 1.090447]\n",
      "354 [D loss: 0.764525, acc.: 50.00%] [G loss: 0.834790]\n",
      "355 [D loss: 0.722400, acc.: 62.50%] [G loss: 1.097109]\n",
      "356 [D loss: 0.896824, acc.: 43.75%] [G loss: 0.912293]\n",
      "357 [D loss: 0.999260, acc.: 31.25%] [G loss: 1.018630]\n",
      "358 [D loss: 0.717012, acc.: 53.12%] [G loss: 0.967388]\n",
      "359 [D loss: 0.696833, acc.: 56.25%] [G loss: 1.004228]\n",
      "360 [D loss: 0.756680, acc.: 46.88%] [G loss: 0.884527]\n",
      "361 [D loss: 0.890692, acc.: 50.00%] [G loss: 1.031385]\n",
      "362 [D loss: 0.825623, acc.: 53.12%] [G loss: 0.999927]\n",
      "363 [D loss: 0.722313, acc.: 50.00%] [G loss: 1.034762]\n",
      "364 [D loss: 1.019208, acc.: 28.12%] [G loss: 0.780445]\n",
      "365 [D loss: 0.711323, acc.: 56.25%] [G loss: 1.106104]\n",
      "366 [D loss: 0.961253, acc.: 31.25%] [G loss: 0.863685]\n",
      "367 [D loss: 1.014039, acc.: 40.62%] [G loss: 0.938118]\n",
      "368 [D loss: 0.823618, acc.: 53.12%] [G loss: 0.898329]\n",
      "369 [D loss: 0.915323, acc.: 40.62%] [G loss: 0.924455]\n",
      "370 [D loss: 0.913662, acc.: 34.38%] [G loss: 0.696694]\n",
      "371 [D loss: 0.683691, acc.: 56.25%] [G loss: 0.835960]\n",
      "372 [D loss: 0.894172, acc.: 43.75%] [G loss: 0.932812]\n",
      "373 [D loss: 0.747833, acc.: 50.00%] [G loss: 1.096048]\n",
      "374 [D loss: 0.909400, acc.: 31.25%] [G loss: 0.798567]\n",
      "375 [D loss: 0.787588, acc.: 50.00%] [G loss: 0.817364]\n",
      "376 [D loss: 0.963235, acc.: 31.25%] [G loss: 0.803061]\n",
      "377 [D loss: 0.917015, acc.: 50.00%] [G loss: 0.948841]\n",
      "378 [D loss: 0.690370, acc.: 53.12%] [G loss: 0.891168]\n",
      "379 [D loss: 0.796256, acc.: 50.00%] [G loss: 0.772295]\n",
      "380 [D loss: 0.918623, acc.: 31.25%] [G loss: 0.850261]\n",
      "381 [D loss: 0.818383, acc.: 53.12%] [G loss: 0.850192]\n",
      "382 [D loss: 0.879589, acc.: 43.75%] [G loss: 1.086663]\n",
      "383 [D loss: 0.786341, acc.: 46.88%] [G loss: 1.066193]\n",
      "384 [D loss: 0.766865, acc.: 56.25%] [G loss: 1.053163]\n",
      "385 [D loss: 0.938814, acc.: 40.62%] [G loss: 0.955203]\n",
      "386 [D loss: 0.715572, acc.: 62.50%] [G loss: 0.908086]\n",
      "387 [D loss: 0.825155, acc.: 37.50%] [G loss: 0.831941]\n",
      "388 [D loss: 0.794500, acc.: 53.12%] [G loss: 0.714760]\n",
      "389 [D loss: 0.673831, acc.: 53.12%] [G loss: 0.594970]\n",
      "390 [D loss: 0.929965, acc.: 34.38%] [G loss: 0.851746]\n",
      "391 [D loss: 0.788389, acc.: 50.00%] [G loss: 0.980991]\n",
      "392 [D loss: 1.052145, acc.: 25.00%] [G loss: 0.898768]\n",
      "393 [D loss: 0.878150, acc.: 43.75%] [G loss: 0.876165]\n",
      "394 [D loss: 0.867850, acc.: 46.88%] [G loss: 1.081785]\n",
      "395 [D loss: 0.927803, acc.: 37.50%] [G loss: 1.004201]\n",
      "396 [D loss: 0.812274, acc.: 53.12%] [G loss: 0.946845]\n",
      "397 [D loss: 0.819618, acc.: 50.00%] [G loss: 0.860361]\n",
      "398 [D loss: 0.737634, acc.: 50.00%] [G loss: 0.966934]\n",
      "399 [D loss: 0.764251, acc.: 53.12%] [G loss: 0.960002]\n",
      "400 [D loss: 0.876952, acc.: 43.75%] [G loss: 0.992835]\n",
      "401 [D loss: 0.790870, acc.: 56.25%] [G loss: 0.944401]\n",
      "402 [D loss: 0.663749, acc.: 59.38%] [G loss: 0.962097]\n",
      "403 [D loss: 0.791351, acc.: 59.38%] [G loss: 0.905516]\n",
      "404 [D loss: 0.731700, acc.: 65.62%] [G loss: 0.780092]\n",
      "405 [D loss: 1.001181, acc.: 43.75%] [G loss: 0.860412]\n",
      "406 [D loss: 0.788260, acc.: 50.00%] [G loss: 0.829709]\n",
      "407 [D loss: 1.072767, acc.: 28.12%] [G loss: 0.877952]\n",
      "408 [D loss: 0.838245, acc.: 50.00%] [G loss: 0.881796]\n",
      "409 [D loss: 0.813136, acc.: 50.00%] [G loss: 0.745882]\n",
      "410 [D loss: 0.808505, acc.: 46.88%] [G loss: 0.955850]\n",
      "411 [D loss: 0.672783, acc.: 56.25%] [G loss: 0.750714]\n",
      "412 [D loss: 0.888066, acc.: 34.38%] [G loss: 0.755573]\n",
      "413 [D loss: 0.708755, acc.: 56.25%] [G loss: 0.830542]\n",
      "414 [D loss: 0.845218, acc.: 46.88%] [G loss: 0.877982]\n",
      "415 [D loss: 0.891028, acc.: 43.75%] [G loss: 0.924230]\n",
      "416 [D loss: 0.618047, acc.: 65.62%] [G loss: 1.111202]\n",
      "417 [D loss: 0.825334, acc.: 40.62%] [G loss: 0.829541]\n",
      "418 [D loss: 0.804730, acc.: 37.50%] [G loss: 0.874930]\n",
      "419 [D loss: 0.747736, acc.: 53.12%] [G loss: 1.029635]\n",
      "420 [D loss: 0.811034, acc.: 50.00%] [G loss: 0.904708]\n",
      "421 [D loss: 0.767097, acc.: 50.00%] [G loss: 1.166354]\n",
      "422 [D loss: 0.956113, acc.: 34.38%] [G loss: 0.830784]\n",
      "423 [D loss: 0.726886, acc.: 40.62%] [G loss: 1.077029]\n",
      "424 [D loss: 0.765609, acc.: 43.75%] [G loss: 1.024876]\n",
      "425 [D loss: 1.015095, acc.: 31.25%] [G loss: 0.876867]\n",
      "426 [D loss: 0.871079, acc.: 43.75%] [G loss: 0.902707]\n",
      "427 [D loss: 0.879757, acc.: 50.00%] [G loss: 0.815697]\n",
      "428 [D loss: 0.816988, acc.: 46.88%] [G loss: 1.073991]\n",
      "429 [D loss: 0.888049, acc.: 46.88%] [G loss: 0.995737]\n",
      "430 [D loss: 0.876792, acc.: 40.62%] [G loss: 0.807341]\n",
      "431 [D loss: 0.764724, acc.: 50.00%] [G loss: 1.033147]\n",
      "432 [D loss: 0.891244, acc.: 37.50%] [G loss: 1.037411]\n",
      "433 [D loss: 0.828352, acc.: 50.00%] [G loss: 1.213485]\n",
      "434 [D loss: 0.862240, acc.: 53.12%] [G loss: 1.009712]\n",
      "435 [D loss: 0.814077, acc.: 43.75%] [G loss: 1.001448]\n",
      "436 [D loss: 0.580197, acc.: 65.62%] [G loss: 0.956455]\n",
      "437 [D loss: 0.822747, acc.: 53.12%] [G loss: 1.068030]\n",
      "438 [D loss: 0.849008, acc.: 34.38%] [G loss: 0.907327]\n",
      "439 [D loss: 0.886338, acc.: 40.62%] [G loss: 0.848633]\n",
      "440 [D loss: 0.771054, acc.: 53.12%] [G loss: 1.050242]\n",
      "441 [D loss: 0.836911, acc.: 46.88%] [G loss: 1.067941]\n",
      "442 [D loss: 0.843845, acc.: 40.62%] [G loss: 0.858254]\n",
      "443 [D loss: 0.773999, acc.: 50.00%] [G loss: 0.961971]\n",
      "444 [D loss: 0.868759, acc.: 40.62%] [G loss: 0.803658]\n",
      "445 [D loss: 0.909565, acc.: 50.00%] [G loss: 1.004866]\n",
      "446 [D loss: 0.773558, acc.: 56.25%] [G loss: 1.049118]\n",
      "447 [D loss: 0.967598, acc.: 31.25%] [G loss: 0.932922]\n",
      "448 [D loss: 0.943180, acc.: 34.38%] [G loss: 0.827152]\n",
      "449 [D loss: 0.952771, acc.: 28.12%] [G loss: 0.812763]\n",
      "450 [D loss: 0.845597, acc.: 37.50%] [G loss: 0.826261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451 [D loss: 0.564687, acc.: 68.75%] [G loss: 0.879614]\n",
      "452 [D loss: 0.698155, acc.: 56.25%] [G loss: 0.833788]\n",
      "453 [D loss: 0.926760, acc.: 28.12%] [G loss: 0.812065]\n",
      "454 [D loss: 0.738143, acc.: 62.50%] [G loss: 0.838694]\n",
      "455 [D loss: 0.865236, acc.: 40.62%] [G loss: 0.906785]\n",
      "456 [D loss: 0.911875, acc.: 37.50%] [G loss: 1.026815]\n",
      "457 [D loss: 0.771083, acc.: 53.12%] [G loss: 1.147197]\n",
      "458 [D loss: 0.872380, acc.: 31.25%] [G loss: 1.189011]\n",
      "459 [D loss: 0.602719, acc.: 62.50%] [G loss: 1.280451]\n",
      "460 [D loss: 0.819551, acc.: 43.75%] [G loss: 1.043511]\n",
      "461 [D loss: 0.928092, acc.: 37.50%] [G loss: 0.829206]\n",
      "462 [D loss: 0.594969, acc.: 65.62%] [G loss: 0.761659]\n",
      "463 [D loss: 0.815826, acc.: 37.50%] [G loss: 0.915486]\n",
      "464 [D loss: 0.812736, acc.: 46.88%] [G loss: 1.037505]\n",
      "465 [D loss: 0.811303, acc.: 43.75%] [G loss: 1.040223]\n",
      "466 [D loss: 0.827406, acc.: 43.75%] [G loss: 0.973311]\n",
      "467 [D loss: 0.809948, acc.: 50.00%] [G loss: 1.063476]\n",
      "468 [D loss: 0.854889, acc.: 56.25%] [G loss: 0.985176]\n",
      "469 [D loss: 0.826003, acc.: 40.62%] [G loss: 0.870065]\n",
      "470 [D loss: 0.767675, acc.: 59.38%] [G loss: 0.865103]\n",
      "471 [D loss: 0.952476, acc.: 37.50%] [G loss: 0.814388]\n",
      "472 [D loss: 0.882268, acc.: 50.00%] [G loss: 0.875844]\n",
      "473 [D loss: 0.722031, acc.: 53.12%] [G loss: 0.824017]\n",
      "474 [D loss: 0.860817, acc.: 43.75%] [G loss: 1.045787]\n",
      "475 [D loss: 0.789582, acc.: 50.00%] [G loss: 1.150490]\n",
      "476 [D loss: 0.894482, acc.: 46.88%] [G loss: 0.959748]\n",
      "477 [D loss: 0.734037, acc.: 53.12%] [G loss: 1.061546]\n",
      "478 [D loss: 0.644353, acc.: 59.38%] [G loss: 0.898171]\n",
      "479 [D loss: 0.676830, acc.: 50.00%] [G loss: 0.922210]\n",
      "480 [D loss: 0.792143, acc.: 53.12%] [G loss: 0.811851]\n",
      "481 [D loss: 0.914094, acc.: 37.50%] [G loss: 0.954985]\n",
      "482 [D loss: 0.930526, acc.: 40.62%] [G loss: 0.917468]\n",
      "483 [D loss: 0.718875, acc.: 50.00%] [G loss: 1.011403]\n",
      "484 [D loss: 0.699819, acc.: 62.50%] [G loss: 0.883461]\n",
      "485 [D loss: 0.820992, acc.: 46.88%] [G loss: 0.929836]\n",
      "486 [D loss: 0.789541, acc.: 46.88%] [G loss: 0.845821]\n",
      "487 [D loss: 0.783353, acc.: 53.12%] [G loss: 1.053626]\n",
      "488 [D loss: 0.834266, acc.: 46.88%] [G loss: 0.837354]\n",
      "489 [D loss: 0.937692, acc.: 34.38%] [G loss: 0.833562]\n",
      "490 [D loss: 0.704293, acc.: 46.88%] [G loss: 0.978908]\n",
      "491 [D loss: 0.733299, acc.: 50.00%] [G loss: 0.899459]\n",
      "492 [D loss: 0.640330, acc.: 62.50%] [G loss: 0.806605]\n",
      "493 [D loss: 0.884238, acc.: 43.75%] [G loss: 0.907055]\n",
      "494 [D loss: 0.844030, acc.: 50.00%] [G loss: 1.057782]\n",
      "495 [D loss: 0.908885, acc.: 25.00%] [G loss: 0.808518]\n",
      "496 [D loss: 0.796598, acc.: 50.00%] [G loss: 0.976366]\n",
      "497 [D loss: 0.964893, acc.: 37.50%] [G loss: 0.757454]\n",
      "498 [D loss: 0.701765, acc.: 53.12%] [G loss: 0.792983]\n",
      "499 [D loss: 0.712624, acc.: 59.38%] [G loss: 1.000571]\n",
      "500 [D loss: 0.764900, acc.: 43.75%] [G loss: 0.786257]\n",
      "501 [D loss: 0.958520, acc.: 40.62%] [G loss: 0.886630]\n",
      "502 [D loss: 0.747632, acc.: 53.12%] [G loss: 0.985497]\n",
      "503 [D loss: 0.759739, acc.: 43.75%] [G loss: 1.000248]\n",
      "504 [D loss: 0.720141, acc.: 62.50%] [G loss: 1.020777]\n",
      "505 [D loss: 0.913033, acc.: 34.38%] [G loss: 0.882892]\n",
      "506 [D loss: 0.830158, acc.: 40.62%] [G loss: 0.795260]\n",
      "507 [D loss: 0.814923, acc.: 50.00%] [G loss: 0.940172]\n",
      "508 [D loss: 0.740084, acc.: 56.25%] [G loss: 0.871593]\n",
      "509 [D loss: 0.682496, acc.: 56.25%] [G loss: 0.987304]\n",
      "510 [D loss: 0.965292, acc.: 43.75%] [G loss: 0.756954]\n",
      "511 [D loss: 0.750950, acc.: 53.12%] [G loss: 0.923284]\n",
      "512 [D loss: 0.738244, acc.: 50.00%] [G loss: 0.781280]\n",
      "513 [D loss: 0.700637, acc.: 62.50%] [G loss: 0.799553]\n",
      "514 [D loss: 0.825465, acc.: 56.25%] [G loss: 0.974394]\n",
      "515 [D loss: 0.912833, acc.: 46.88%] [G loss: 1.046727]\n",
      "516 [D loss: 0.798259, acc.: 43.75%] [G loss: 0.719703]\n",
      "517 [D loss: 0.860981, acc.: 40.62%] [G loss: 1.031017]\n",
      "518 [D loss: 0.771880, acc.: 53.12%] [G loss: 0.996163]\n",
      "519 [D loss: 0.752437, acc.: 43.75%] [G loss: 1.056709]\n",
      "520 [D loss: 0.935888, acc.: 34.38%] [G loss: 1.142458]\n",
      "521 [D loss: 0.836389, acc.: 43.75%] [G loss: 0.995042]\n",
      "522 [D loss: 0.755070, acc.: 46.88%] [G loss: 0.967588]\n",
      "523 [D loss: 0.756411, acc.: 59.38%] [G loss: 0.742270]\n",
      "524 [D loss: 0.716229, acc.: 46.88%] [G loss: 1.004023]\n",
      "525 [D loss: 0.635944, acc.: 65.62%] [G loss: 0.941586]\n",
      "526 [D loss: 0.816977, acc.: 40.62%] [G loss: 0.903994]\n",
      "527 [D loss: 0.751104, acc.: 50.00%] [G loss: 1.018552]\n",
      "528 [D loss: 0.857612, acc.: 43.75%] [G loss: 1.016012]\n",
      "529 [D loss: 0.753523, acc.: 53.12%] [G loss: 1.057107]\n",
      "530 [D loss: 0.752023, acc.: 59.38%] [G loss: 0.924774]\n",
      "531 [D loss: 0.904917, acc.: 34.38%] [G loss: 0.916484]\n",
      "532 [D loss: 0.795702, acc.: 43.75%] [G loss: 0.883563]\n",
      "533 [D loss: 0.658024, acc.: 50.00%] [G loss: 0.859316]\n",
      "534 [D loss: 0.766551, acc.: 46.88%] [G loss: 0.790265]\n",
      "535 [D loss: 0.791037, acc.: 50.00%] [G loss: 0.708145]\n",
      "536 [D loss: 0.799775, acc.: 50.00%] [G loss: 0.700793]\n",
      "537 [D loss: 0.980944, acc.: 37.50%] [G loss: 1.010605]\n",
      "538 [D loss: 0.703111, acc.: 59.38%] [G loss: 0.843103]\n",
      "539 [D loss: 0.806763, acc.: 50.00%] [G loss: 0.834792]\n",
      "540 [D loss: 0.769647, acc.: 40.62%] [G loss: 0.992932]\n",
      "541 [D loss: 0.740440, acc.: 50.00%] [G loss: 0.788835]\n",
      "542 [D loss: 0.604409, acc.: 68.75%] [G loss: 1.081760]\n",
      "543 [D loss: 0.888237, acc.: 40.62%] [G loss: 0.837459]\n",
      "544 [D loss: 0.750793, acc.: 53.12%] [G loss: 1.195292]\n",
      "545 [D loss: 0.620355, acc.: 68.75%] [G loss: 0.931127]\n",
      "546 [D loss: 0.876086, acc.: 37.50%] [G loss: 0.867686]\n",
      "547 [D loss: 0.782031, acc.: 53.12%] [G loss: 0.869595]\n",
      "548 [D loss: 0.720870, acc.: 53.12%] [G loss: 0.866098]\n",
      "549 [D loss: 0.743466, acc.: 46.88%] [G loss: 0.877648]\n",
      "550 [D loss: 0.806731, acc.: 53.12%] [G loss: 0.943768]\n",
      "551 [D loss: 0.792212, acc.: 43.75%] [G loss: 1.096650]\n",
      "552 [D loss: 0.801189, acc.: 46.88%] [G loss: 0.912481]\n",
      "553 [D loss: 0.715531, acc.: 46.88%] [G loss: 1.001579]\n",
      "554 [D loss: 0.765534, acc.: 53.12%] [G loss: 1.015066]\n",
      "555 [D loss: 0.811710, acc.: 40.62%] [G loss: 0.864123]\n",
      "556 [D loss: 0.761812, acc.: 40.62%] [G loss: 0.917261]\n",
      "557 [D loss: 0.800840, acc.: 40.62%] [G loss: 1.131100]\n",
      "558 [D loss: 0.933920, acc.: 43.75%] [G loss: 0.910150]\n",
      "559 [D loss: 0.670493, acc.: 50.00%] [G loss: 1.107325]\n",
      "560 [D loss: 0.641673, acc.: 56.25%] [G loss: 0.976902]\n",
      "561 [D loss: 0.798621, acc.: 46.88%] [G loss: 1.004888]\n",
      "562 [D loss: 0.728078, acc.: 68.75%] [G loss: 0.963170]\n",
      "563 [D loss: 0.686480, acc.: 50.00%] [G loss: 0.888772]\n",
      "564 [D loss: 0.862974, acc.: 34.38%] [G loss: 0.894321]\n",
      "565 [D loss: 0.813410, acc.: 50.00%] [G loss: 1.020599]\n",
      "566 [D loss: 0.826019, acc.: 50.00%] [G loss: 1.087816]\n",
      "567 [D loss: 0.787200, acc.: 56.25%] [G loss: 0.779127]\n",
      "568 [D loss: 0.937607, acc.: 37.50%] [G loss: 0.940920]\n",
      "569 [D loss: 0.984498, acc.: 28.12%] [G loss: 0.999856]\n",
      "570 [D loss: 0.756423, acc.: 46.88%] [G loss: 0.882358]\n",
      "571 [D loss: 0.777144, acc.: 50.00%] [G loss: 0.823463]\n",
      "572 [D loss: 0.849575, acc.: 43.75%] [G loss: 1.175139]\n",
      "573 [D loss: 0.664034, acc.: 56.25%] [G loss: 0.999643]\n",
      "574 [D loss: 0.812188, acc.: 43.75%] [G loss: 0.963165]\n",
      "575 [D loss: 0.738505, acc.: 53.12%] [G loss: 0.753929]\n",
      "576 [D loss: 0.717514, acc.: 62.50%] [G loss: 0.860300]\n",
      "577 [D loss: 0.723057, acc.: 53.12%] [G loss: 0.734769]\n",
      "578 [D loss: 0.725708, acc.: 53.12%] [G loss: 0.865443]\n",
      "579 [D loss: 0.841133, acc.: 34.38%] [G loss: 1.044181]\n",
      "580 [D loss: 0.931205, acc.: 43.75%] [G loss: 0.978665]\n",
      "581 [D loss: 0.823817, acc.: 53.12%] [G loss: 0.988846]\n",
      "582 [D loss: 0.738058, acc.: 56.25%] [G loss: 0.957910]\n",
      "583 [D loss: 0.871017, acc.: 37.50%] [G loss: 0.891854]\n",
      "584 [D loss: 0.825989, acc.: 43.75%] [G loss: 1.041367]\n",
      "585 [D loss: 0.671285, acc.: 56.25%] [G loss: 1.051022]\n",
      "586 [D loss: 0.725860, acc.: 56.25%] [G loss: 0.869819]\n",
      "587 [D loss: 0.851615, acc.: 43.75%] [G loss: 0.854334]\n",
      "588 [D loss: 0.885882, acc.: 40.62%] [G loss: 0.833369]\n",
      "589 [D loss: 0.823360, acc.: 40.62%] [G loss: 1.053279]\n",
      "590 [D loss: 0.867449, acc.: 43.75%] [G loss: 0.794645]\n",
      "591 [D loss: 0.755998, acc.: 53.12%] [G loss: 0.911401]\n",
      "592 [D loss: 0.780713, acc.: 46.88%] [G loss: 1.008825]\n",
      "593 [D loss: 0.953666, acc.: 28.12%] [G loss: 1.014609]\n",
      "594 [D loss: 0.874504, acc.: 34.38%] [G loss: 0.800920]\n",
      "595 [D loss: 0.698642, acc.: 56.25%] [G loss: 0.802540]\n",
      "596 [D loss: 0.868835, acc.: 43.75%] [G loss: 0.975197]\n",
      "597 [D loss: 0.696491, acc.: 53.12%] [G loss: 1.034549]\n",
      "598 [D loss: 0.794598, acc.: 53.12%] [G loss: 0.957094]\n",
      "599 [D loss: 0.698011, acc.: 62.50%] [G loss: 0.829373]\n",
      "600 [D loss: 0.736038, acc.: 50.00%] [G loss: 0.902825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601 [D loss: 0.844074, acc.: 40.62%] [G loss: 1.040664]\n",
      "602 [D loss: 0.678228, acc.: 59.38%] [G loss: 0.993440]\n",
      "603 [D loss: 0.843323, acc.: 46.88%] [G loss: 0.883795]\n",
      "604 [D loss: 0.835433, acc.: 46.88%] [G loss: 0.983314]\n",
      "605 [D loss: 0.810059, acc.: 43.75%] [G loss: 0.883666]\n",
      "606 [D loss: 0.684153, acc.: 56.25%] [G loss: 0.798817]\n",
      "607 [D loss: 0.712030, acc.: 50.00%] [G loss: 0.941178]\n",
      "608 [D loss: 0.709936, acc.: 56.25%] [G loss: 0.998260]\n",
      "609 [D loss: 0.817102, acc.: 46.88%] [G loss: 0.982737]\n",
      "610 [D loss: 0.890321, acc.: 43.75%] [G loss: 1.041978]\n",
      "611 [D loss: 0.738179, acc.: 46.88%] [G loss: 0.989702]\n",
      "612 [D loss: 0.821666, acc.: 53.12%] [G loss: 0.902773]\n",
      "613 [D loss: 0.808482, acc.: 43.75%] [G loss: 0.923298]\n",
      "614 [D loss: 0.814465, acc.: 50.00%] [G loss: 1.038082]\n",
      "615 [D loss: 0.757267, acc.: 53.12%] [G loss: 0.879972]\n",
      "616 [D loss: 0.774989, acc.: 37.50%] [G loss: 0.918246]\n",
      "617 [D loss: 0.741929, acc.: 46.88%] [G loss: 0.892204]\n",
      "618 [D loss: 0.766255, acc.: 50.00%] [G loss: 0.865722]\n",
      "619 [D loss: 1.025660, acc.: 25.00%] [G loss: 0.662187]\n",
      "620 [D loss: 0.907798, acc.: 40.62%] [G loss: 0.990084]\n",
      "621 [D loss: 0.694170, acc.: 56.25%] [G loss: 0.964102]\n",
      "622 [D loss: 0.890550, acc.: 21.88%] [G loss: 0.978194]\n",
      "623 [D loss: 0.724101, acc.: 59.38%] [G loss: 0.844618]\n",
      "624 [D loss: 0.908079, acc.: 43.75%] [G loss: 0.901795]\n",
      "625 [D loss: 0.780220, acc.: 46.88%] [G loss: 0.947693]\n",
      "626 [D loss: 0.927082, acc.: 31.25%] [G loss: 0.704472]\n",
      "627 [D loss: 0.794029, acc.: 40.62%] [G loss: 1.076747]\n",
      "628 [D loss: 0.705382, acc.: 46.88%] [G loss: 0.931211]\n",
      "629 [D loss: 0.693536, acc.: 62.50%] [G loss: 1.084842]\n",
      "630 [D loss: 0.710787, acc.: 46.88%] [G loss: 0.931660]\n",
      "631 [D loss: 0.839380, acc.: 40.62%] [G loss: 0.979008]\n",
      "632 [D loss: 0.957743, acc.: 34.38%] [G loss: 1.001644]\n",
      "633 [D loss: 0.899319, acc.: 40.62%] [G loss: 1.110984]\n",
      "634 [D loss: 0.723754, acc.: 59.38%] [G loss: 1.106289]\n",
      "635 [D loss: 0.860069, acc.: 46.88%] [G loss: 0.970741]\n",
      "636 [D loss: 0.731723, acc.: 59.38%] [G loss: 0.934866]\n",
      "637 [D loss: 0.990701, acc.: 25.00%] [G loss: 0.860965]\n",
      "638 [D loss: 0.807315, acc.: 43.75%] [G loss: 0.840252]\n",
      "639 [D loss: 0.859556, acc.: 50.00%] [G loss: 0.982406]\n",
      "640 [D loss: 0.742835, acc.: 62.50%] [G loss: 0.940776]\n",
      "641 [D loss: 0.575370, acc.: 75.00%] [G loss: 0.781205]\n",
      "642 [D loss: 0.869763, acc.: 46.88%] [G loss: 0.866648]\n",
      "643 [D loss: 0.877222, acc.: 43.75%] [G loss: 0.896701]\n",
      "644 [D loss: 0.804448, acc.: 46.88%] [G loss: 0.986978]\n",
      "645 [D loss: 0.618965, acc.: 65.62%] [G loss: 1.197695]\n",
      "646 [D loss: 0.841380, acc.: 46.88%] [G loss: 0.951606]\n",
      "647 [D loss: 0.743696, acc.: 50.00%] [G loss: 0.996973]\n",
      "648 [D loss: 0.624690, acc.: 59.38%] [G loss: 0.991261]\n",
      "649 [D loss: 0.812312, acc.: 40.62%] [G loss: 0.926279]\n",
      "650 [D loss: 0.598709, acc.: 75.00%] [G loss: 0.951528]\n",
      "651 [D loss: 0.729034, acc.: 53.12%] [G loss: 0.814032]\n",
      "652 [D loss: 0.801534, acc.: 46.88%] [G loss: 1.124104]\n",
      "653 [D loss: 0.610727, acc.: 68.75%] [G loss: 1.073751]\n",
      "654 [D loss: 0.783253, acc.: 34.38%] [G loss: 0.807189]\n",
      "655 [D loss: 0.785360, acc.: 50.00%] [G loss: 0.969071]\n",
      "656 [D loss: 0.719318, acc.: 50.00%] [G loss: 0.938340]\n",
      "657 [D loss: 0.781976, acc.: 53.12%] [G loss: 0.868128]\n",
      "658 [D loss: 0.743321, acc.: 53.12%] [G loss: 0.880751]\n",
      "659 [D loss: 0.689931, acc.: 53.12%] [G loss: 0.972349]\n",
      "660 [D loss: 0.679005, acc.: 53.12%] [G loss: 0.778585]\n",
      "661 [D loss: 0.838694, acc.: 50.00%] [G loss: 0.873490]\n",
      "662 [D loss: 0.789310, acc.: 40.62%] [G loss: 0.857543]\n",
      "663 [D loss: 0.647017, acc.: 59.38%] [G loss: 0.918079]\n",
      "664 [D loss: 0.956115, acc.: 40.62%] [G loss: 1.025229]\n",
      "665 [D loss: 0.740887, acc.: 53.12%] [G loss: 0.935942]\n",
      "666 [D loss: 0.781797, acc.: 43.75%] [G loss: 0.895269]\n",
      "667 [D loss: 0.708074, acc.: 59.38%] [G loss: 0.883164]\n",
      "668 [D loss: 0.849785, acc.: 37.50%] [G loss: 0.857183]\n",
      "669 [D loss: 0.760653, acc.: 50.00%] [G loss: 1.018154]\n",
      "670 [D loss: 0.668400, acc.: 65.62%] [G loss: 1.092976]\n",
      "671 [D loss: 0.782595, acc.: 43.75%] [G loss: 1.098763]\n",
      "672 [D loss: 0.693792, acc.: 59.38%] [G loss: 1.028246]\n",
      "673 [D loss: 0.713337, acc.: 50.00%] [G loss: 0.976308]\n",
      "674 [D loss: 0.768813, acc.: 46.88%] [G loss: 0.959673]\n",
      "675 [D loss: 0.769640, acc.: 50.00%] [G loss: 0.908850]\n",
      "676 [D loss: 0.828067, acc.: 50.00%] [G loss: 0.802491]\n",
      "677 [D loss: 0.712750, acc.: 56.25%] [G loss: 0.777746]\n",
      "678 [D loss: 0.677001, acc.: 50.00%] [G loss: 0.851052]\n",
      "679 [D loss: 0.721285, acc.: 46.88%] [G loss: 0.904358]\n",
      "680 [D loss: 0.810250, acc.: 53.12%] [G loss: 1.018835]\n",
      "681 [D loss: 0.685641, acc.: 71.88%] [G loss: 0.936583]\n",
      "682 [D loss: 0.820144, acc.: 50.00%] [G loss: 1.045489]\n",
      "683 [D loss: 0.815770, acc.: 43.75%] [G loss: 0.739442]\n",
      "684 [D loss: 0.842907, acc.: 56.25%] [G loss: 0.863760]\n",
      "685 [D loss: 0.628172, acc.: 71.88%] [G loss: 0.960661]\n",
      "686 [D loss: 0.809969, acc.: 50.00%] [G loss: 0.908156]\n",
      "687 [D loss: 0.755098, acc.: 53.12%] [G loss: 0.978683]\n",
      "688 [D loss: 0.794647, acc.: 56.25%] [G loss: 0.921687]\n",
      "689 [D loss: 0.649692, acc.: 59.38%] [G loss: 0.899515]\n",
      "690 [D loss: 0.771493, acc.: 50.00%] [G loss: 1.030896]\n",
      "691 [D loss: 0.864583, acc.: 31.25%] [G loss: 0.917726]\n",
      "692 [D loss: 0.569714, acc.: 59.38%] [G loss: 0.744376]\n",
      "693 [D loss: 0.816315, acc.: 34.38%] [G loss: 0.949843]\n",
      "694 [D loss: 0.704505, acc.: 68.75%] [G loss: 0.962022]\n",
      "695 [D loss: 0.693548, acc.: 62.50%] [G loss: 1.092962]\n",
      "696 [D loss: 0.939891, acc.: 40.62%] [G loss: 0.936004]\n",
      "697 [D loss: 0.978922, acc.: 28.12%] [G loss: 0.835752]\n",
      "698 [D loss: 0.838065, acc.: 53.12%] [G loss: 0.851876]\n",
      "699 [D loss: 0.757588, acc.: 43.75%] [G loss: 0.803243]\n",
      "700 [D loss: 0.859321, acc.: 43.75%] [G loss: 1.002442]\n",
      "701 [D loss: 0.803750, acc.: 50.00%] [G loss: 0.978226]\n",
      "702 [D loss: 0.646052, acc.: 65.62%] [G loss: 1.069585]\n",
      "703 [D loss: 0.844741, acc.: 50.00%] [G loss: 0.920637]\n",
      "704 [D loss: 0.653522, acc.: 62.50%] [G loss: 0.794706]\n",
      "705 [D loss: 0.753067, acc.: 46.88%] [G loss: 1.025111]\n",
      "706 [D loss: 0.731281, acc.: 53.12%] [G loss: 1.092569]\n",
      "707 [D loss: 0.751843, acc.: 59.38%] [G loss: 0.967400]\n",
      "708 [D loss: 0.690873, acc.: 62.50%] [G loss: 1.007619]\n",
      "709 [D loss: 0.851964, acc.: 46.88%] [G loss: 0.742643]\n",
      "710 [D loss: 0.999293, acc.: 37.50%] [G loss: 0.874093]\n",
      "711 [D loss: 0.782737, acc.: 46.88%] [G loss: 0.944163]\n",
      "712 [D loss: 0.815064, acc.: 43.75%] [G loss: 0.697377]\n",
      "713 [D loss: 0.674047, acc.: 62.50%] [G loss: 0.958147]\n",
      "714 [D loss: 0.861129, acc.: 43.75%] [G loss: 0.836699]\n",
      "715 [D loss: 0.799505, acc.: 31.25%] [G loss: 0.805314]\n",
      "716 [D loss: 0.722072, acc.: 59.38%] [G loss: 0.918606]\n",
      "717 [D loss: 0.716685, acc.: 56.25%] [G loss: 1.076284]\n",
      "718 [D loss: 0.808577, acc.: 46.88%] [G loss: 0.801495]\n",
      "719 [D loss: 0.781566, acc.: 40.62%] [G loss: 0.944753]\n",
      "720 [D loss: 0.797970, acc.: 53.12%] [G loss: 0.819666]\n",
      "721 [D loss: 0.654794, acc.: 62.50%] [G loss: 0.834564]\n",
      "722 [D loss: 0.737247, acc.: 53.12%] [G loss: 0.958193]\n",
      "723 [D loss: 0.727253, acc.: 59.38%] [G loss: 0.900996]\n",
      "724 [D loss: 0.792816, acc.: 40.62%] [G loss: 0.777237]\n",
      "725 [D loss: 0.846493, acc.: 37.50%] [G loss: 0.782003]\n",
      "726 [D loss: 0.694751, acc.: 56.25%] [G loss: 0.893116]\n",
      "727 [D loss: 0.936251, acc.: 50.00%] [G loss: 0.820269]\n",
      "728 [D loss: 0.698718, acc.: 62.50%] [G loss: 1.022723]\n",
      "729 [D loss: 0.785359, acc.: 43.75%] [G loss: 0.922644]\n",
      "730 [D loss: 0.865320, acc.: 34.38%] [G loss: 0.848319]\n",
      "731 [D loss: 0.832129, acc.: 34.38%] [G loss: 0.905068]\n",
      "732 [D loss: 0.643478, acc.: 68.75%] [G loss: 0.951090]\n",
      "733 [D loss: 0.745498, acc.: 50.00%] [G loss: 1.074880]\n",
      "734 [D loss: 0.820895, acc.: 43.75%] [G loss: 0.933873]\n",
      "735 [D loss: 0.793227, acc.: 46.88%] [G loss: 0.956126]\n",
      "736 [D loss: 0.829815, acc.: 40.62%] [G loss: 0.852710]\n",
      "737 [D loss: 0.793230, acc.: 59.38%] [G loss: 0.915171]\n",
      "738 [D loss: 0.700671, acc.: 56.25%] [G loss: 0.811606]\n",
      "739 [D loss: 0.668121, acc.: 59.38%] [G loss: 0.982542]\n",
      "740 [D loss: 0.881625, acc.: 40.62%] [G loss: 0.869702]\n",
      "741 [D loss: 0.796714, acc.: 53.12%] [G loss: 0.762429]\n",
      "742 [D loss: 0.667016, acc.: 53.12%] [G loss: 0.887879]\n",
      "743 [D loss: 0.646325, acc.: 68.75%] [G loss: 0.889642]\n",
      "744 [D loss: 0.701458, acc.: 46.88%] [G loss: 1.014746]\n",
      "745 [D loss: 0.626482, acc.: 65.62%] [G loss: 1.010823]\n",
      "746 [D loss: 0.988213, acc.: 28.12%] [G loss: 0.955411]\n",
      "747 [D loss: 0.778763, acc.: 43.75%] [G loss: 0.927123]\n",
      "748 [D loss: 0.800029, acc.: 43.75%] [G loss: 0.767049]\n",
      "749 [D loss: 0.748291, acc.: 56.25%] [G loss: 0.803776]\n",
      "750 [D loss: 0.708917, acc.: 59.38%] [G loss: 0.805751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "751 [D loss: 0.669143, acc.: 59.38%] [G loss: 1.077587]\n",
      "752 [D loss: 0.748564, acc.: 50.00%] [G loss: 1.021429]\n",
      "753 [D loss: 0.794448, acc.: 50.00%] [G loss: 1.001646]\n",
      "754 [D loss: 0.829202, acc.: 40.62%] [G loss: 0.952267]\n",
      "755 [D loss: 0.734504, acc.: 59.38%] [G loss: 1.014950]\n",
      "756 [D loss: 0.750173, acc.: 46.88%] [G loss: 0.896929]\n",
      "757 [D loss: 0.757075, acc.: 56.25%] [G loss: 0.820738]\n",
      "758 [D loss: 0.766146, acc.: 40.62%] [G loss: 0.789742]\n",
      "759 [D loss: 0.736167, acc.: 62.50%] [G loss: 0.893753]\n",
      "760 [D loss: 0.779810, acc.: 43.75%] [G loss: 0.880179]\n",
      "761 [D loss: 1.009141, acc.: 37.50%] [G loss: 0.858683]\n",
      "762 [D loss: 0.789930, acc.: 50.00%] [G loss: 1.020809]\n",
      "763 [D loss: 0.762673, acc.: 53.12%] [G loss: 0.990550]\n",
      "764 [D loss: 0.833675, acc.: 37.50%] [G loss: 0.973320]\n",
      "765 [D loss: 0.810632, acc.: 40.62%] [G loss: 0.783085]\n",
      "766 [D loss: 0.726945, acc.: 53.12%] [G loss: 0.856160]\n",
      "767 [D loss: 0.700914, acc.: 53.12%] [G loss: 0.901650]\n",
      "768 [D loss: 0.615228, acc.: 65.62%] [G loss: 0.872647]\n",
      "769 [D loss: 0.704905, acc.: 50.00%] [G loss: 0.837132]\n",
      "770 [D loss: 0.802773, acc.: 37.50%] [G loss: 0.929168]\n",
      "771 [D loss: 0.766018, acc.: 40.62%] [G loss: 1.163070]\n",
      "772 [D loss: 0.826048, acc.: 43.75%] [G loss: 0.807736]\n",
      "773 [D loss: 0.841961, acc.: 46.88%] [G loss: 0.957129]\n",
      "774 [D loss: 0.749428, acc.: 50.00%] [G loss: 0.838125]\n",
      "775 [D loss: 0.716988, acc.: 46.88%] [G loss: 0.748153]\n",
      "776 [D loss: 0.787315, acc.: 40.62%] [G loss: 0.876781]\n",
      "777 [D loss: 0.719316, acc.: 50.00%] [G loss: 0.904094]\n",
      "778 [D loss: 0.847035, acc.: 40.62%] [G loss: 0.926918]\n",
      "779 [D loss: 0.758398, acc.: 62.50%] [G loss: 0.881838]\n",
      "780 [D loss: 0.798978, acc.: 40.62%] [G loss: 0.893510]\n",
      "781 [D loss: 0.662267, acc.: 56.25%] [G loss: 0.999605]\n",
      "782 [D loss: 0.771105, acc.: 40.62%] [G loss: 0.953827]\n",
      "783 [D loss: 0.761322, acc.: 59.38%] [G loss: 0.908404]\n",
      "784 [D loss: 0.868909, acc.: 40.62%] [G loss: 0.880385]\n",
      "785 [D loss: 0.712786, acc.: 53.12%] [G loss: 0.903705]\n",
      "786 [D loss: 0.703561, acc.: 59.38%] [G loss: 0.822428]\n",
      "787 [D loss: 0.785655, acc.: 59.38%] [G loss: 1.035347]\n",
      "788 [D loss: 0.674178, acc.: 50.00%] [G loss: 1.022724]\n",
      "789 [D loss: 0.758925, acc.: 46.88%] [G loss: 0.888855]\n",
      "790 [D loss: 0.675482, acc.: 56.25%] [G loss: 0.855717]\n",
      "791 [D loss: 0.841431, acc.: 40.62%] [G loss: 0.895589]\n",
      "792 [D loss: 0.682448, acc.: 68.75%] [G loss: 1.131214]\n",
      "793 [D loss: 0.752764, acc.: 40.62%] [G loss: 0.921949]\n",
      "794 [D loss: 0.743043, acc.: 53.12%] [G loss: 1.007280]\n",
      "795 [D loss: 0.687755, acc.: 53.12%] [G loss: 0.829963]\n",
      "796 [D loss: 0.867205, acc.: 28.12%] [G loss: 0.767614]\n",
      "797 [D loss: 0.750877, acc.: 43.75%] [G loss: 0.723902]\n",
      "798 [D loss: 0.845848, acc.: 43.75%] [G loss: 0.787792]\n",
      "799 [D loss: 0.692243, acc.: 53.12%] [G loss: 0.856518]\n",
      "800 [D loss: 0.723554, acc.: 50.00%] [G loss: 0.827038]\n",
      "801 [D loss: 0.798749, acc.: 46.88%] [G loss: 0.846265]\n",
      "802 [D loss: 0.703176, acc.: 56.25%] [G loss: 0.858982]\n",
      "803 [D loss: 0.723909, acc.: 50.00%] [G loss: 0.924295]\n",
      "804 [D loss: 0.670098, acc.: 71.88%] [G loss: 0.827767]\n",
      "805 [D loss: 0.720851, acc.: 46.88%] [G loss: 0.903987]\n",
      "806 [D loss: 0.772685, acc.: 46.88%] [G loss: 0.789992]\n",
      "807 [D loss: 0.799052, acc.: 50.00%] [G loss: 0.851639]\n",
      "808 [D loss: 0.755683, acc.: 53.12%] [G loss: 0.951626]\n",
      "809 [D loss: 0.771639, acc.: 53.12%] [G loss: 0.910076]\n",
      "810 [D loss: 0.660016, acc.: 50.00%] [G loss: 1.015274]\n",
      "811 [D loss: 0.804064, acc.: 50.00%] [G loss: 1.024034]\n",
      "812 [D loss: 0.818806, acc.: 40.62%] [G loss: 0.988325]\n",
      "813 [D loss: 0.883677, acc.: 31.25%] [G loss: 0.898908]\n",
      "814 [D loss: 0.699788, acc.: 50.00%] [G loss: 1.010406]\n",
      "815 [D loss: 0.802147, acc.: 43.75%] [G loss: 0.822217]\n",
      "816 [D loss: 0.851688, acc.: 31.25%] [G loss: 0.984246]\n",
      "817 [D loss: 0.727018, acc.: 53.12%] [G loss: 0.807961]\n",
      "818 [D loss: 0.843328, acc.: 40.62%] [G loss: 0.717705]\n",
      "819 [D loss: 0.744949, acc.: 43.75%] [G loss: 0.957883]\n",
      "820 [D loss: 0.671396, acc.: 59.38%] [G loss: 0.957771]\n",
      "821 [D loss: 0.970462, acc.: 31.25%] [G loss: 0.914967]\n",
      "822 [D loss: 0.793644, acc.: 46.88%] [G loss: 0.975744]\n",
      "823 [D loss: 0.692030, acc.: 53.12%] [G loss: 0.976797]\n",
      "824 [D loss: 0.734525, acc.: 56.25%] [G loss: 0.965133]\n",
      "825 [D loss: 0.804197, acc.: 43.75%] [G loss: 1.038467]\n",
      "826 [D loss: 0.809259, acc.: 43.75%] [G loss: 0.962514]\n",
      "827 [D loss: 0.721448, acc.: 53.12%] [G loss: 0.911733]\n",
      "828 [D loss: 0.744563, acc.: 43.75%] [G loss: 0.930930]\n",
      "829 [D loss: 0.657856, acc.: 62.50%] [G loss: 1.008060]\n",
      "830 [D loss: 0.699256, acc.: 62.50%] [G loss: 0.784838]\n",
      "831 [D loss: 0.678815, acc.: 56.25%] [G loss: 0.765717]\n",
      "832 [D loss: 0.680568, acc.: 62.50%] [G loss: 0.776308]\n",
      "833 [D loss: 0.733655, acc.: 59.38%] [G loss: 0.792166]\n",
      "834 [D loss: 0.674332, acc.: 56.25%] [G loss: 1.012548]\n",
      "835 [D loss: 0.710585, acc.: 56.25%] [G loss: 1.004960]\n",
      "836 [D loss: 0.905080, acc.: 34.38%] [G loss: 0.940203]\n",
      "837 [D loss: 0.693351, acc.: 50.00%] [G loss: 0.906185]\n",
      "838 [D loss: 0.733182, acc.: 50.00%] [G loss: 0.963991]\n",
      "839 [D loss: 0.699630, acc.: 50.00%] [G loss: 0.982097]\n",
      "840 [D loss: 0.700721, acc.: 53.12%] [G loss: 0.919488]\n",
      "841 [D loss: 0.744560, acc.: 50.00%] [G loss: 1.133310]\n",
      "842 [D loss: 0.667360, acc.: 62.50%] [G loss: 1.044292]\n",
      "843 [D loss: 0.957881, acc.: 31.25%] [G loss: 0.859873]\n",
      "844 [D loss: 0.756853, acc.: 46.88%] [G loss: 0.970375]\n",
      "845 [D loss: 0.799306, acc.: 40.62%] [G loss: 0.855420]\n",
      "846 [D loss: 0.834172, acc.: 50.00%] [G loss: 1.017470]\n",
      "847 [D loss: 0.715416, acc.: 62.50%] [G loss: 1.037687]\n",
      "848 [D loss: 0.718632, acc.: 53.12%] [G loss: 0.993840]\n",
      "849 [D loss: 0.897614, acc.: 34.38%] [G loss: 0.906586]\n",
      "850 [D loss: 0.824624, acc.: 40.62%] [G loss: 0.859498]\n",
      "851 [D loss: 0.531866, acc.: 75.00%] [G loss: 0.824519]\n",
      "852 [D loss: 0.764272, acc.: 53.12%] [G loss: 0.855545]\n",
      "853 [D loss: 0.699896, acc.: 50.00%] [G loss: 0.896954]\n",
      "854 [D loss: 0.785068, acc.: 46.88%] [G loss: 0.879780]\n",
      "855 [D loss: 0.714623, acc.: 59.38%] [G loss: 0.763587]\n",
      "856 [D loss: 0.723419, acc.: 53.12%] [G loss: 1.014170]\n",
      "857 [D loss: 0.738200, acc.: 56.25%] [G loss: 0.870476]\n",
      "858 [D loss: 0.753316, acc.: 43.75%] [G loss: 0.982768]\n",
      "859 [D loss: 0.722664, acc.: 56.25%] [G loss: 1.096463]\n",
      "860 [D loss: 0.696502, acc.: 62.50%] [G loss: 0.960552]\n",
      "861 [D loss: 0.661423, acc.: 56.25%] [G loss: 1.033399]\n",
      "862 [D loss: 0.813898, acc.: 50.00%] [G loss: 1.060470]\n",
      "863 [D loss: 0.908031, acc.: 43.75%] [G loss: 0.863836]\n",
      "864 [D loss: 0.709413, acc.: 56.25%] [G loss: 1.097935]\n",
      "865 [D loss: 0.584819, acc.: 65.62%] [G loss: 0.993155]\n",
      "866 [D loss: 0.840214, acc.: 37.50%] [G loss: 0.821022]\n",
      "867 [D loss: 0.680175, acc.: 59.38%] [G loss: 1.002037]\n",
      "868 [D loss: 0.772760, acc.: 50.00%] [G loss: 0.903435]\n",
      "869 [D loss: 0.758501, acc.: 50.00%] [G loss: 0.990165]\n",
      "870 [D loss: 0.829077, acc.: 46.88%] [G loss: 0.924838]\n",
      "871 [D loss: 0.863070, acc.: 25.00%] [G loss: 0.889014]\n",
      "872 [D loss: 0.684604, acc.: 56.25%] [G loss: 0.985775]\n",
      "873 [D loss: 0.869608, acc.: 53.12%] [G loss: 0.845287]\n",
      "874 [D loss: 0.845857, acc.: 37.50%] [G loss: 1.002730]\n",
      "875 [D loss: 0.579762, acc.: 65.62%] [G loss: 0.968849]\n",
      "876 [D loss: 0.811571, acc.: 43.75%] [G loss: 0.919304]\n",
      "877 [D loss: 0.752760, acc.: 53.12%] [G loss: 0.870719]\n",
      "878 [D loss: 0.779160, acc.: 56.25%] [G loss: 0.982707]\n",
      "879 [D loss: 0.676777, acc.: 68.75%] [G loss: 0.899674]\n",
      "880 [D loss: 0.803636, acc.: 50.00%] [G loss: 0.761729]\n",
      "881 [D loss: 0.655638, acc.: 65.62%] [G loss: 0.991965]\n",
      "882 [D loss: 0.725873, acc.: 53.12%] [G loss: 0.951599]\n",
      "883 [D loss: 0.758873, acc.: 56.25%] [G loss: 0.737129]\n",
      "884 [D loss: 0.728556, acc.: 40.62%] [G loss: 0.896740]\n",
      "885 [D loss: 0.696556, acc.: 53.12%] [G loss: 0.902218]\n",
      "886 [D loss: 0.704939, acc.: 56.25%] [G loss: 0.947896]\n",
      "887 [D loss: 0.585752, acc.: 71.88%] [G loss: 1.091241]\n",
      "888 [D loss: 0.811760, acc.: 50.00%] [G loss: 0.986818]\n",
      "889 [D loss: 0.602896, acc.: 65.62%] [G loss: 0.945205]\n",
      "890 [D loss: 0.824428, acc.: 46.88%] [G loss: 0.926805]\n",
      "891 [D loss: 0.614868, acc.: 65.62%] [G loss: 0.904516]\n",
      "892 [D loss: 0.763934, acc.: 46.88%] [G loss: 0.985977]\n",
      "893 [D loss: 0.844446, acc.: 43.75%] [G loss: 0.951330]\n",
      "894 [D loss: 0.747463, acc.: 37.50%] [G loss: 0.995980]\n",
      "895 [D loss: 0.843660, acc.: 40.62%] [G loss: 0.947065]\n",
      "896 [D loss: 0.764526, acc.: 40.62%] [G loss: 1.206160]\n",
      "897 [D loss: 0.735245, acc.: 53.12%] [G loss: 0.870792]\n",
      "898 [D loss: 0.824167, acc.: 53.12%] [G loss: 0.682515]\n",
      "899 [D loss: 0.686525, acc.: 50.00%] [G loss: 0.947181]\n",
      "900 [D loss: 0.687962, acc.: 53.12%] [G loss: 1.003074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901 [D loss: 0.805773, acc.: 46.88%] [G loss: 0.880099]\n",
      "902 [D loss: 0.841963, acc.: 40.62%] [G loss: 0.915545]\n",
      "903 [D loss: 0.834787, acc.: 40.62%] [G loss: 1.063383]\n",
      "904 [D loss: 0.648931, acc.: 68.75%] [G loss: 0.939006]\n",
      "905 [D loss: 0.708981, acc.: 62.50%] [G loss: 1.058537]\n",
      "906 [D loss: 0.793892, acc.: 40.62%] [G loss: 0.937756]\n",
      "907 [D loss: 0.739876, acc.: 56.25%] [G loss: 1.054374]\n",
      "908 [D loss: 0.749382, acc.: 53.12%] [G loss: 1.004531]\n",
      "909 [D loss: 0.798939, acc.: 40.62%] [G loss: 0.912733]\n",
      "910 [D loss: 0.772642, acc.: 50.00%] [G loss: 0.879463]\n",
      "911 [D loss: 0.814748, acc.: 43.75%] [G loss: 1.050684]\n",
      "912 [D loss: 0.792562, acc.: 43.75%] [G loss: 0.984359]\n",
      "913 [D loss: 0.613628, acc.: 78.12%] [G loss: 1.018261]\n",
      "914 [D loss: 0.747097, acc.: 53.12%] [G loss: 0.831286]\n",
      "915 [D loss: 0.728940, acc.: 46.88%] [G loss: 1.044839]\n",
      "916 [D loss: 0.840440, acc.: 31.25%] [G loss: 1.073790]\n",
      "917 [D loss: 0.718337, acc.: 50.00%] [G loss: 1.023030]\n",
      "918 [D loss: 0.800343, acc.: 53.12%] [G loss: 0.976382]\n",
      "919 [D loss: 0.683069, acc.: 59.38%] [G loss: 0.921326]\n",
      "920 [D loss: 0.804591, acc.: 43.75%] [G loss: 1.063025]\n",
      "921 [D loss: 0.714588, acc.: 46.88%] [G loss: 0.888809]\n",
      "922 [D loss: 0.797111, acc.: 59.38%] [G loss: 0.871255]\n",
      "923 [D loss: 0.737520, acc.: 46.88%] [G loss: 1.179472]\n",
      "924 [D loss: 0.688756, acc.: 50.00%] [G loss: 1.022133]\n",
      "925 [D loss: 0.820130, acc.: 43.75%] [G loss: 1.012687]\n",
      "926 [D loss: 0.662020, acc.: 59.38%] [G loss: 1.010407]\n",
      "927 [D loss: 0.758219, acc.: 43.75%] [G loss: 0.918988]\n",
      "928 [D loss: 0.646327, acc.: 65.62%] [G loss: 0.987379]\n",
      "929 [D loss: 0.645382, acc.: 62.50%] [G loss: 0.998323]\n",
      "930 [D loss: 0.708354, acc.: 46.88%] [G loss: 0.936733]\n",
      "931 [D loss: 0.722637, acc.: 59.38%] [G loss: 0.915006]\n",
      "932 [D loss: 0.642657, acc.: 62.50%] [G loss: 1.095893]\n",
      "933 [D loss: 0.766517, acc.: 46.88%] [G loss: 1.109682]\n",
      "934 [D loss: 0.723692, acc.: 62.50%] [G loss: 0.959289]\n",
      "935 [D loss: 0.805587, acc.: 40.62%] [G loss: 0.924208]\n",
      "936 [D loss: 0.705714, acc.: 53.12%] [G loss: 0.886303]\n",
      "937 [D loss: 0.694669, acc.: 59.38%] [G loss: 0.888112]\n",
      "938 [D loss: 0.704744, acc.: 56.25%] [G loss: 0.926451]\n",
      "939 [D loss: 0.836332, acc.: 40.62%] [G loss: 0.974739]\n",
      "940 [D loss: 0.810474, acc.: 56.25%] [G loss: 0.941327]\n",
      "941 [D loss: 0.812790, acc.: 40.62%] [G loss: 0.789381]\n",
      "942 [D loss: 0.714226, acc.: 53.12%] [G loss: 0.928790]\n",
      "943 [D loss: 0.732139, acc.: 50.00%] [G loss: 0.947416]\n",
      "944 [D loss: 0.767677, acc.: 43.75%] [G loss: 0.970163]\n",
      "945 [D loss: 0.767919, acc.: 53.12%] [G loss: 0.849178]\n",
      "946 [D loss: 0.913681, acc.: 31.25%] [G loss: 0.839090]\n",
      "947 [D loss: 0.662319, acc.: 65.62%] [G loss: 0.921768]\n",
      "948 [D loss: 0.712820, acc.: 59.38%] [G loss: 0.957102]\n",
      "949 [D loss: 0.740171, acc.: 59.38%] [G loss: 0.831970]\n",
      "950 [D loss: 0.765109, acc.: 43.75%] [G loss: 0.948095]\n",
      "951 [D loss: 0.757339, acc.: 53.12%] [G loss: 1.154308]\n",
      "952 [D loss: 0.675507, acc.: 53.12%] [G loss: 0.902311]\n",
      "953 [D loss: 0.807303, acc.: 40.62%] [G loss: 0.977327]\n",
      "954 [D loss: 0.710651, acc.: 62.50%] [G loss: 1.126346]\n",
      "955 [D loss: 0.823008, acc.: 53.12%] [G loss: 0.860094]\n",
      "956 [D loss: 0.713318, acc.: 56.25%] [G loss: 1.015844]\n",
      "957 [D loss: 0.645921, acc.: 68.75%] [G loss: 0.761189]\n",
      "958 [D loss: 0.715953, acc.: 46.88%] [G loss: 0.919036]\n",
      "959 [D loss: 0.671589, acc.: 59.38%] [G loss: 0.847786]\n",
      "960 [D loss: 0.938069, acc.: 31.25%] [G loss: 1.069603]\n",
      "961 [D loss: 0.789549, acc.: 46.88%] [G loss: 0.935624]\n",
      "962 [D loss: 0.886193, acc.: 40.62%] [G loss: 0.941680]\n",
      "963 [D loss: 0.638273, acc.: 62.50%] [G loss: 0.885364]\n",
      "964 [D loss: 0.736593, acc.: 40.62%] [G loss: 0.964955]\n",
      "965 [D loss: 0.904826, acc.: 40.62%] [G loss: 0.926421]\n",
      "966 [D loss: 0.659781, acc.: 53.12%] [G loss: 1.066659]\n",
      "967 [D loss: 0.634114, acc.: 71.88%] [G loss: 1.021247]\n",
      "968 [D loss: 0.695621, acc.: 59.38%] [G loss: 0.913695]\n",
      "969 [D loss: 0.788777, acc.: 53.12%] [G loss: 0.855176]\n",
      "970 [D loss: 0.825684, acc.: 43.75%] [G loss: 0.952802]\n",
      "971 [D loss: 0.696387, acc.: 59.38%] [G loss: 0.900623]\n",
      "972 [D loss: 0.612786, acc.: 62.50%] [G loss: 0.796105]\n",
      "973 [D loss: 0.725048, acc.: 62.50%] [G loss: 0.961425]\n",
      "974 [D loss: 0.824026, acc.: 46.88%] [G loss: 0.780851]\n",
      "975 [D loss: 0.774378, acc.: 56.25%] [G loss: 0.726275]\n",
      "976 [D loss: 0.690992, acc.: 56.25%] [G loss: 0.997169]\n",
      "977 [D loss: 0.571427, acc.: 71.88%] [G loss: 0.886677]\n",
      "978 [D loss: 0.753683, acc.: 59.38%] [G loss: 0.866060]\n",
      "979 [D loss: 0.715360, acc.: 50.00%] [G loss: 0.809622]\n",
      "980 [D loss: 0.789808, acc.: 37.50%] [G loss: 0.947705]\n",
      "981 [D loss: 0.714092, acc.: 50.00%] [G loss: 1.082049]\n",
      "982 [D loss: 0.776720, acc.: 46.88%] [G loss: 0.890017]\n",
      "983 [D loss: 0.649524, acc.: 62.50%] [G loss: 1.105627]\n",
      "984 [D loss: 0.680023, acc.: 59.38%] [G loss: 0.874772]\n",
      "985 [D loss: 0.759353, acc.: 50.00%] [G loss: 0.908040]\n",
      "986 [D loss: 0.811490, acc.: 37.50%] [G loss: 0.812792]\n",
      "987 [D loss: 0.821105, acc.: 37.50%] [G loss: 0.776671]\n",
      "988 [D loss: 0.891532, acc.: 46.88%] [G loss: 0.895258]\n",
      "989 [D loss: 0.779645, acc.: 46.88%] [G loss: 0.815397]\n",
      "990 [D loss: 0.755524, acc.: 53.12%] [G loss: 0.869599]\n",
      "991 [D loss: 0.860779, acc.: 34.38%] [G loss: 0.844257]\n",
      "992 [D loss: 0.675346, acc.: 53.12%] [G loss: 0.909879]\n",
      "993 [D loss: 0.794405, acc.: 40.62%] [G loss: 0.818681]\n",
      "994 [D loss: 0.605003, acc.: 68.75%] [G loss: 0.976934]\n",
      "995 [D loss: 0.750533, acc.: 56.25%] [G loss: 0.798161]\n",
      "996 [D loss: 0.696512, acc.: 46.88%] [G loss: 0.977566]\n",
      "997 [D loss: 0.810404, acc.: 50.00%] [G loss: 0.979342]\n",
      "998 [D loss: 0.693841, acc.: 59.38%] [G loss: 0.828287]\n",
      "999 [D loss: 0.744940, acc.: 56.25%] [G loss: 0.717197]\n",
      "1000 [D loss: 0.717164, acc.: 46.88%] [G loss: 0.978427]\n",
      "1001 [D loss: 0.897357, acc.: 31.25%] [G loss: 0.889241]\n",
      "1002 [D loss: 0.664400, acc.: 62.50%] [G loss: 0.921038]\n",
      "1003 [D loss: 0.782963, acc.: 46.88%] [G loss: 0.763329]\n",
      "1004 [D loss: 0.722494, acc.: 53.12%] [G loss: 0.971805]\n",
      "1005 [D loss: 0.629128, acc.: 59.38%] [G loss: 0.944269]\n",
      "1006 [D loss: 0.756571, acc.: 62.50%] [G loss: 0.789113]\n",
      "1007 [D loss: 0.809619, acc.: 43.75%] [G loss: 0.800674]\n",
      "1008 [D loss: 0.745883, acc.: 59.38%] [G loss: 0.817210]\n",
      "1009 [D loss: 0.754709, acc.: 46.88%] [G loss: 0.904120]\n",
      "1010 [D loss: 0.825359, acc.: 43.75%] [G loss: 0.890033]\n",
      "1011 [D loss: 0.702125, acc.: 56.25%] [G loss: 0.779400]\n",
      "1012 [D loss: 0.794970, acc.: 46.88%] [G loss: 0.816203]\n",
      "1013 [D loss: 0.930164, acc.: 31.25%] [G loss: 0.828819]\n",
      "1014 [D loss: 0.766175, acc.: 56.25%] [G loss: 0.910409]\n",
      "1015 [D loss: 0.723823, acc.: 56.25%] [G loss: 0.929712]\n",
      "1016 [D loss: 0.680129, acc.: 59.38%] [G loss: 0.733891]\n",
      "1017 [D loss: 0.746777, acc.: 53.12%] [G loss: 0.992172]\n",
      "1018 [D loss: 0.745846, acc.: 46.88%] [G loss: 1.080971]\n",
      "1019 [D loss: 0.738603, acc.: 53.12%] [G loss: 1.092258]\n",
      "1020 [D loss: 0.756937, acc.: 50.00%] [G loss: 0.880688]\n",
      "1021 [D loss: 0.728959, acc.: 50.00%] [G loss: 0.913204]\n",
      "1022 [D loss: 0.909015, acc.: 40.62%] [G loss: 0.840169]\n",
      "1023 [D loss: 0.803498, acc.: 37.50%] [G loss: 0.952820]\n",
      "1024 [D loss: 0.856618, acc.: 34.38%] [G loss: 0.976674]\n",
      "1025 [D loss: 0.656973, acc.: 56.25%] [G loss: 0.901879]\n",
      "1026 [D loss: 0.781354, acc.: 56.25%] [G loss: 0.884965]\n",
      "1027 [D loss: 0.862099, acc.: 37.50%] [G loss: 0.880855]\n",
      "1028 [D loss: 0.737975, acc.: 56.25%] [G loss: 0.950293]\n",
      "1029 [D loss: 0.728354, acc.: 50.00%] [G loss: 0.941605]\n",
      "1030 [D loss: 0.816514, acc.: 43.75%] [G loss: 0.973418]\n",
      "1031 [D loss: 0.834277, acc.: 46.88%] [G loss: 0.921047]\n",
      "1032 [D loss: 0.783626, acc.: 46.88%] [G loss: 1.037545]\n",
      "1033 [D loss: 0.734279, acc.: 43.75%] [G loss: 0.986106]\n",
      "1034 [D loss: 0.770329, acc.: 43.75%] [G loss: 0.931112]\n",
      "1035 [D loss: 0.732735, acc.: 56.25%] [G loss: 0.869512]\n",
      "1036 [D loss: 0.747593, acc.: 53.12%] [G loss: 0.802973]\n",
      "1037 [D loss: 0.725072, acc.: 53.12%] [G loss: 0.939385]\n",
      "1038 [D loss: 0.684271, acc.: 62.50%] [G loss: 0.943723]\n",
      "1039 [D loss: 0.802998, acc.: 40.62%] [G loss: 0.957396]\n",
      "1040 [D loss: 0.749997, acc.: 56.25%] [G loss: 0.758366]\n",
      "1041 [D loss: 0.688661, acc.: 65.62%] [G loss: 0.986195]\n",
      "1042 [D loss: 0.691482, acc.: 59.38%] [G loss: 0.904036]\n",
      "1043 [D loss: 0.638634, acc.: 59.38%] [G loss: 0.939210]\n",
      "1044 [D loss: 0.714677, acc.: 56.25%] [G loss: 0.834244]\n",
      "1045 [D loss: 0.768689, acc.: 46.88%] [G loss: 0.996497]\n",
      "1046 [D loss: 0.665944, acc.: 50.00%] [G loss: 0.940201]\n",
      "1047 [D loss: 0.756456, acc.: 50.00%] [G loss: 0.828104]\n",
      "1048 [D loss: 0.640065, acc.: 62.50%] [G loss: 0.976768]\n",
      "1049 [D loss: 0.766408, acc.: 53.12%] [G loss: 0.913465]\n",
      "1050 [D loss: 0.856745, acc.: 46.88%] [G loss: 0.801726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1051 [D loss: 0.758702, acc.: 59.38%] [G loss: 0.960357]\n",
      "1052 [D loss: 0.626934, acc.: 71.88%] [G loss: 0.838841]\n",
      "1053 [D loss: 0.698399, acc.: 56.25%] [G loss: 0.975618]\n",
      "1054 [D loss: 0.718616, acc.: 50.00%] [G loss: 0.756481]\n",
      "1055 [D loss: 0.749007, acc.: 56.25%] [G loss: 0.902100]\n",
      "1056 [D loss: 0.740789, acc.: 68.75%] [G loss: 1.092706]\n",
      "1057 [D loss: 0.732739, acc.: 65.62%] [G loss: 1.011984]\n",
      "1058 [D loss: 0.800597, acc.: 43.75%] [G loss: 0.839484]\n",
      "1059 [D loss: 0.571110, acc.: 68.75%] [G loss: 0.844804]\n",
      "1060 [D loss: 0.624616, acc.: 68.75%] [G loss: 1.025043]\n",
      "1061 [D loss: 0.826424, acc.: 40.62%] [G loss: 0.840274]\n",
      "1062 [D loss: 0.755385, acc.: 46.88%] [G loss: 0.853069]\n",
      "1063 [D loss: 0.662757, acc.: 68.75%] [G loss: 0.995262]\n",
      "1064 [D loss: 0.688546, acc.: 62.50%] [G loss: 0.959428]\n",
      "1065 [D loss: 0.781752, acc.: 43.75%] [G loss: 1.110161]\n",
      "1066 [D loss: 0.561707, acc.: 81.25%] [G loss: 0.837591]\n",
      "1067 [D loss: 0.815264, acc.: 37.50%] [G loss: 0.847651]\n",
      "1068 [D loss: 0.760349, acc.: 46.88%] [G loss: 0.778520]\n",
      "1069 [D loss: 0.560829, acc.: 65.62%] [G loss: 0.795200]\n",
      "1070 [D loss: 0.677510, acc.: 59.38%] [G loss: 0.994405]\n",
      "1071 [D loss: 0.720095, acc.: 53.12%] [G loss: 1.051443]\n",
      "1072 [D loss: 0.837266, acc.: 46.88%] [G loss: 0.986927]\n",
      "1073 [D loss: 0.745057, acc.: 40.62%] [G loss: 0.807319]\n",
      "1074 [D loss: 0.803848, acc.: 43.75%] [G loss: 1.006183]\n",
      "1075 [D loss: 0.626165, acc.: 65.62%] [G loss: 1.128440]\n",
      "1076 [D loss: 0.863845, acc.: 37.50%] [G loss: 1.005379]\n",
      "1077 [D loss: 0.853029, acc.: 34.38%] [G loss: 0.894869]\n",
      "1078 [D loss: 0.695747, acc.: 46.88%] [G loss: 0.898227]\n",
      "1079 [D loss: 0.687868, acc.: 53.12%] [G loss: 0.951917]\n",
      "1080 [D loss: 0.856501, acc.: 40.62%] [G loss: 0.863877]\n",
      "1081 [D loss: 0.668913, acc.: 59.38%] [G loss: 1.025042]\n",
      "1082 [D loss: 0.844114, acc.: 28.12%] [G loss: 1.127133]\n",
      "1083 [D loss: 0.650529, acc.: 62.50%] [G loss: 0.989886]\n",
      "1084 [D loss: 0.687076, acc.: 65.62%] [G loss: 1.020443]\n",
      "1085 [D loss: 0.695724, acc.: 59.38%] [G loss: 0.878797]\n",
      "1086 [D loss: 0.802139, acc.: 50.00%] [G loss: 0.786455]\n",
      "1087 [D loss: 0.681347, acc.: 59.38%] [G loss: 0.900985]\n",
      "1088 [D loss: 0.656470, acc.: 59.38%] [G loss: 0.860117]\n",
      "1089 [D loss: 0.706415, acc.: 59.38%] [G loss: 0.954303]\n",
      "1090 [D loss: 0.724423, acc.: 43.75%] [G loss: 0.774400]\n",
      "1091 [D loss: 0.651221, acc.: 56.25%] [G loss: 0.900364]\n",
      "1092 [D loss: 0.734876, acc.: 56.25%] [G loss: 1.019056]\n",
      "1093 [D loss: 0.733367, acc.: 50.00%] [G loss: 0.935361]\n",
      "1094 [D loss: 0.677430, acc.: 56.25%] [G loss: 1.029901]\n",
      "1095 [D loss: 0.902698, acc.: 34.38%] [G loss: 0.973889]\n",
      "1096 [D loss: 0.692531, acc.: 56.25%] [G loss: 1.020418]\n",
      "1097 [D loss: 0.768784, acc.: 53.12%] [G loss: 0.930265]\n",
      "1098 [D loss: 0.728382, acc.: 50.00%] [G loss: 0.870113]\n",
      "1099 [D loss: 0.660338, acc.: 59.38%] [G loss: 0.988791]\n",
      "1100 [D loss: 0.590162, acc.: 62.50%] [G loss: 0.965613]\n",
      "1101 [D loss: 0.632462, acc.: 62.50%] [G loss: 0.906284]\n",
      "1102 [D loss: 0.765120, acc.: 50.00%] [G loss: 1.065574]\n",
      "1103 [D loss: 0.846835, acc.: 34.38%] [G loss: 0.772943]\n",
      "1104 [D loss: 0.724349, acc.: 56.25%] [G loss: 0.993509]\n",
      "1105 [D loss: 0.778410, acc.: 53.12%] [G loss: 0.814067]\n",
      "1106 [D loss: 0.941157, acc.: 37.50%] [G loss: 1.002836]\n",
      "1107 [D loss: 0.843294, acc.: 46.88%] [G loss: 0.987265]\n",
      "1108 [D loss: 0.793244, acc.: 56.25%] [G loss: 0.914993]\n",
      "1109 [D loss: 0.727560, acc.: 53.12%] [G loss: 1.120115]\n",
      "1110 [D loss: 0.724394, acc.: 50.00%] [G loss: 1.031312]\n",
      "1111 [D loss: 0.705320, acc.: 59.38%] [G loss: 1.011284]\n",
      "1112 [D loss: 0.700027, acc.: 65.62%] [G loss: 0.815986]\n",
      "1113 [D loss: 0.844119, acc.: 46.88%] [G loss: 0.782183]\n",
      "1114 [D loss: 0.667924, acc.: 62.50%] [G loss: 1.018217]\n",
      "1115 [D loss: 0.758620, acc.: 59.38%] [G loss: 0.842544]\n",
      "1116 [D loss: 0.720510, acc.: 46.88%] [G loss: 0.914733]\n",
      "1117 [D loss: 0.965927, acc.: 21.88%] [G loss: 0.765428]\n",
      "1118 [D loss: 0.752789, acc.: 43.75%] [G loss: 0.854959]\n",
      "1119 [D loss: 0.680474, acc.: 59.38%] [G loss: 0.806590]\n",
      "1120 [D loss: 0.790085, acc.: 37.50%] [G loss: 0.795677]\n",
      "1121 [D loss: 0.698122, acc.: 59.38%] [G loss: 0.800148]\n",
      "1122 [D loss: 0.753960, acc.: 50.00%] [G loss: 0.904028]\n",
      "1123 [D loss: 0.638270, acc.: 65.62%] [G loss: 0.887417]\n",
      "1124 [D loss: 0.772698, acc.: 62.50%] [G loss: 0.903871]\n",
      "1125 [D loss: 0.755477, acc.: 59.38%] [G loss: 0.872157]\n",
      "1126 [D loss: 0.687494, acc.: 56.25%] [G loss: 0.926253]\n",
      "1127 [D loss: 0.709972, acc.: 50.00%] [G loss: 1.008478]\n",
      "1128 [D loss: 0.736750, acc.: 56.25%] [G loss: 1.062003]\n",
      "1129 [D loss: 0.775500, acc.: 46.88%] [G loss: 0.905519]\n",
      "1130 [D loss: 0.641216, acc.: 68.75%] [G loss: 1.047571]\n",
      "1131 [D loss: 0.830829, acc.: 28.12%] [G loss: 0.787795]\n",
      "1132 [D loss: 0.735674, acc.: 56.25%] [G loss: 0.901546]\n",
      "1133 [D loss: 0.594566, acc.: 75.00%] [G loss: 1.020867]\n",
      "1134 [D loss: 0.700393, acc.: 53.12%] [G loss: 0.924155]\n",
      "1135 [D loss: 0.731364, acc.: 62.50%] [G loss: 0.827509]\n",
      "1136 [D loss: 0.757473, acc.: 46.88%] [G loss: 0.959730]\n",
      "1137 [D loss: 0.586184, acc.: 65.62%] [G loss: 0.981919]\n",
      "1138 [D loss: 0.718209, acc.: 43.75%] [G loss: 0.951399]\n",
      "1139 [D loss: 0.773882, acc.: 40.62%] [G loss: 0.884834]\n",
      "1140 [D loss: 0.652707, acc.: 75.00%] [G loss: 0.854361]\n",
      "1141 [D loss: 0.707961, acc.: 46.88%] [G loss: 0.798944]\n",
      "1142 [D loss: 0.702355, acc.: 62.50%] [G loss: 1.055295]\n",
      "1143 [D loss: 0.723038, acc.: 59.38%] [G loss: 1.039165]\n",
      "1144 [D loss: 0.847744, acc.: 40.62%] [G loss: 0.936981]\n",
      "1145 [D loss: 0.728559, acc.: 53.12%] [G loss: 0.894718]\n",
      "1146 [D loss: 0.709985, acc.: 50.00%] [G loss: 0.888079]\n",
      "1147 [D loss: 0.728719, acc.: 53.12%] [G loss: 1.064526]\n",
      "1148 [D loss: 0.708703, acc.: 46.88%] [G loss: 0.869291]\n",
      "1149 [D loss: 0.723754, acc.: 53.12%] [G loss: 1.090134]\n",
      "1150 [D loss: 0.688741, acc.: 53.12%] [G loss: 0.837422]\n",
      "1151 [D loss: 0.765307, acc.: 37.50%] [G loss: 0.888644]\n",
      "1152 [D loss: 0.760011, acc.: 53.12%] [G loss: 0.908370]\n",
      "1153 [D loss: 0.656213, acc.: 56.25%] [G loss: 0.945593]\n",
      "1154 [D loss: 0.762931, acc.: 40.62%] [G loss: 1.020154]\n",
      "1155 [D loss: 0.703470, acc.: 65.62%] [G loss: 0.991633]\n",
      "1156 [D loss: 0.624278, acc.: 68.75%] [G loss: 0.984837]\n",
      "1157 [D loss: 0.693009, acc.: 65.62%] [G loss: 0.994001]\n",
      "1158 [D loss: 0.708891, acc.: 56.25%] [G loss: 0.956928]\n",
      "1159 [D loss: 0.724058, acc.: 56.25%] [G loss: 0.748121]\n",
      "1160 [D loss: 0.656902, acc.: 62.50%] [G loss: 0.887634]\n",
      "1161 [D loss: 0.620609, acc.: 68.75%] [G loss: 0.930421]\n",
      "1162 [D loss: 0.712460, acc.: 62.50%] [G loss: 0.985142]\n",
      "1163 [D loss: 0.772883, acc.: 40.62%] [G loss: 0.924820]\n",
      "1164 [D loss: 0.721741, acc.: 50.00%] [G loss: 0.901849]\n",
      "1165 [D loss: 0.703415, acc.: 53.12%] [G loss: 0.748698]\n",
      "1166 [D loss: 0.550575, acc.: 68.75%] [G loss: 0.976572]\n",
      "1167 [D loss: 0.652674, acc.: 53.12%] [G loss: 0.818990]\n",
      "1168 [D loss: 0.811923, acc.: 40.62%] [G loss: 0.967884]\n",
      "1169 [D loss: 0.743167, acc.: 50.00%] [G loss: 1.162595]\n",
      "1170 [D loss: 0.723211, acc.: 50.00%] [G loss: 1.053832]\n",
      "1171 [D loss: 0.798792, acc.: 43.75%] [G loss: 0.817581]\n",
      "1172 [D loss: 0.682661, acc.: 62.50%] [G loss: 1.011448]\n",
      "1173 [D loss: 0.679135, acc.: 65.62%] [G loss: 0.877278]\n",
      "1174 [D loss: 0.749735, acc.: 37.50%] [G loss: 0.845090]\n",
      "1175 [D loss: 0.707854, acc.: 56.25%] [G loss: 0.921221]\n",
      "1176 [D loss: 0.692831, acc.: 53.12%] [G loss: 0.889299]\n",
      "1177 [D loss: 0.597492, acc.: 62.50%] [G loss: 0.937928]\n",
      "1178 [D loss: 0.795312, acc.: 46.88%] [G loss: 0.829596]\n",
      "1179 [D loss: 0.646968, acc.: 65.62%] [G loss: 0.957533]\n",
      "1180 [D loss: 0.784025, acc.: 43.75%] [G loss: 0.964046]\n",
      "1181 [D loss: 0.814045, acc.: 34.38%] [G loss: 0.947017]\n",
      "1182 [D loss: 0.699427, acc.: 59.38%] [G loss: 0.786750]\n",
      "1183 [D loss: 0.637392, acc.: 62.50%] [G loss: 0.890334]\n",
      "1184 [D loss: 0.701577, acc.: 46.88%] [G loss: 0.816787]\n",
      "1185 [D loss: 0.654027, acc.: 56.25%] [G loss: 0.917665]\n",
      "1186 [D loss: 0.792067, acc.: 40.62%] [G loss: 0.776171]\n",
      "1187 [D loss: 0.674335, acc.: 59.38%] [G loss: 0.983252]\n",
      "1188 [D loss: 0.703203, acc.: 56.25%] [G loss: 0.858552]\n",
      "1189 [D loss: 0.710109, acc.: 56.25%] [G loss: 0.999763]\n",
      "1190 [D loss: 0.644708, acc.: 59.38%] [G loss: 0.986494]\n",
      "1191 [D loss: 0.707629, acc.: 50.00%] [G loss: 0.995351]\n",
      "1192 [D loss: 0.780726, acc.: 53.12%] [G loss: 0.952480]\n",
      "1193 [D loss: 0.623891, acc.: 71.88%] [G loss: 1.075689]\n",
      "1194 [D loss: 0.781596, acc.: 53.12%] [G loss: 0.979485]\n",
      "1195 [D loss: 0.831635, acc.: 34.38%] [G loss: 1.100968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1196 [D loss: 0.663200, acc.: 62.50%] [G loss: 1.015478]\n",
      "1197 [D loss: 0.693101, acc.: 59.38%] [G loss: 0.900995]\n",
      "1198 [D loss: 0.823031, acc.: 37.50%] [G loss: 0.831017]\n",
      "1199 [D loss: 0.728226, acc.: 59.38%] [G loss: 0.896684]\n",
      "1200 [D loss: 0.827910, acc.: 46.88%] [G loss: 0.888012]\n",
      "1201 [D loss: 0.802978, acc.: 50.00%] [G loss: 0.855495]\n",
      "1202 [D loss: 0.609157, acc.: 59.38%] [G loss: 0.963837]\n",
      "1203 [D loss: 0.742217, acc.: 59.38%] [G loss: 0.958601]\n",
      "1204 [D loss: 0.695911, acc.: 65.62%] [G loss: 0.878500]\n",
      "1205 [D loss: 0.644594, acc.: 62.50%] [G loss: 0.906100]\n",
      "1206 [D loss: 0.771676, acc.: 46.88%] [G loss: 0.752431]\n",
      "1207 [D loss: 0.733099, acc.: 53.12%] [G loss: 1.042437]\n",
      "1208 [D loss: 0.651114, acc.: 65.62%] [G loss: 0.966604]\n",
      "1209 [D loss: 0.742991, acc.: 50.00%] [G loss: 0.985792]\n",
      "1210 [D loss: 0.728574, acc.: 53.12%] [G loss: 0.891964]\n",
      "1211 [D loss: 0.761366, acc.: 46.88%] [G loss: 0.956938]\n",
      "1212 [D loss: 0.806498, acc.: 43.75%] [G loss: 0.918888]\n",
      "1213 [D loss: 0.812444, acc.: 40.62%] [G loss: 0.722791]\n",
      "1214 [D loss: 0.696281, acc.: 53.12%] [G loss: 0.835552]\n",
      "1215 [D loss: 0.707129, acc.: 59.38%] [G loss: 0.858023]\n",
      "1216 [D loss: 0.619639, acc.: 65.62%] [G loss: 0.799470]\n",
      "1217 [D loss: 0.650128, acc.: 53.12%] [G loss: 0.828820]\n",
      "1218 [D loss: 0.719764, acc.: 53.12%] [G loss: 0.948943]\n",
      "1219 [D loss: 0.646947, acc.: 59.38%] [G loss: 0.778060]\n",
      "1220 [D loss: 0.838320, acc.: 46.88%] [G loss: 0.862129]\n",
      "1221 [D loss: 0.764484, acc.: 50.00%] [G loss: 0.852122]\n",
      "1222 [D loss: 0.906864, acc.: 31.25%] [G loss: 0.854663]\n",
      "1223 [D loss: 0.718088, acc.: 53.12%] [G loss: 0.977362]\n",
      "1224 [D loss: 0.713018, acc.: 53.12%] [G loss: 0.901784]\n",
      "1225 [D loss: 0.774156, acc.: 43.75%] [G loss: 0.927460]\n",
      "1226 [D loss: 0.835223, acc.: 31.25%] [G loss: 0.785275]\n",
      "1227 [D loss: 0.717350, acc.: 53.12%] [G loss: 0.956163]\n",
      "1228 [D loss: 0.753713, acc.: 50.00%] [G loss: 0.871459]\n",
      "1229 [D loss: 0.657167, acc.: 68.75%] [G loss: 0.935595]\n",
      "1230 [D loss: 0.676580, acc.: 53.12%] [G loss: 0.760694]\n",
      "1231 [D loss: 0.685689, acc.: 53.12%] [G loss: 0.976653]\n",
      "1232 [D loss: 0.706095, acc.: 53.12%] [G loss: 0.893331]\n",
      "1233 [D loss: 0.792583, acc.: 43.75%] [G loss: 0.852970]\n",
      "1234 [D loss: 0.746619, acc.: 53.12%] [G loss: 0.965583]\n",
      "1235 [D loss: 0.790953, acc.: 53.12%] [G loss: 0.820459]\n",
      "1236 [D loss: 0.753972, acc.: 56.25%] [G loss: 0.930378]\n",
      "1237 [D loss: 0.741375, acc.: 46.88%] [G loss: 0.912539]\n",
      "1238 [D loss: 0.790498, acc.: 56.25%] [G loss: 1.006196]\n",
      "1239 [D loss: 0.744797, acc.: 59.38%] [G loss: 0.936338]\n",
      "1240 [D loss: 0.705337, acc.: 56.25%] [G loss: 1.114972]\n",
      "1241 [D loss: 0.709653, acc.: 43.75%] [G loss: 0.937033]\n",
      "1242 [D loss: 0.778375, acc.: 46.88%] [G loss: 1.025583]\n",
      "1243 [D loss: 0.678492, acc.: 56.25%] [G loss: 1.033643]\n",
      "1244 [D loss: 0.695566, acc.: 53.12%] [G loss: 0.842281]\n",
      "1245 [D loss: 0.813154, acc.: 40.62%] [G loss: 0.920259]\n",
      "1246 [D loss: 0.692268, acc.: 53.12%] [G loss: 0.909581]\n",
      "1247 [D loss: 0.621976, acc.: 59.38%] [G loss: 0.992108]\n",
      "1248 [D loss: 0.711146, acc.: 53.12%] [G loss: 0.796867]\n",
      "1249 [D loss: 0.701052, acc.: 56.25%] [G loss: 1.039219]\n",
      "1250 [D loss: 0.739557, acc.: 46.88%] [G loss: 1.008031]\n",
      "1251 [D loss: 0.736387, acc.: 50.00%] [G loss: 0.885271]\n",
      "1252 [D loss: 0.691038, acc.: 56.25%] [G loss: 0.853149]\n",
      "1253 [D loss: 0.689670, acc.: 59.38%] [G loss: 1.138657]\n",
      "1254 [D loss: 0.804604, acc.: 43.75%] [G loss: 1.028063]\n",
      "1255 [D loss: 0.835162, acc.: 43.75%] [G loss: 0.970653]\n",
      "1256 [D loss: 0.846460, acc.: 46.88%] [G loss: 1.033192]\n",
      "1257 [D loss: 0.659423, acc.: 56.25%] [G loss: 1.032902]\n",
      "1258 [D loss: 0.703516, acc.: 65.62%] [G loss: 1.301812]\n",
      "1259 [D loss: 0.716414, acc.: 46.88%] [G loss: 1.113840]\n",
      "1260 [D loss: 0.852871, acc.: 43.75%] [G loss: 0.910238]\n",
      "1261 [D loss: 0.648475, acc.: 68.75%] [G loss: 1.081899]\n",
      "1262 [D loss: 0.772633, acc.: 40.62%] [G loss: 0.972430]\n",
      "1263 [D loss: 0.637066, acc.: 65.62%] [G loss: 0.935889]\n",
      "1264 [D loss: 0.678391, acc.: 50.00%] [G loss: 1.001266]\n",
      "1265 [D loss: 0.685446, acc.: 59.38%] [G loss: 0.862543]\n",
      "1266 [D loss: 0.623992, acc.: 62.50%] [G loss: 0.882506]\n",
      "1267 [D loss: 0.693048, acc.: 53.12%] [G loss: 0.819672]\n",
      "1268 [D loss: 0.777786, acc.: 46.88%] [G loss: 0.854180]\n",
      "1269 [D loss: 0.685272, acc.: 68.75%] [G loss: 1.011709]\n",
      "1270 [D loss: 0.844232, acc.: 37.50%] [G loss: 0.928319]\n",
      "1271 [D loss: 0.895157, acc.: 37.50%] [G loss: 0.728080]\n",
      "1272 [D loss: 0.741340, acc.: 50.00%] [G loss: 0.888430]\n",
      "1273 [D loss: 0.723086, acc.: 56.25%] [G loss: 0.841099]\n",
      "1274 [D loss: 0.717028, acc.: 46.88%] [G loss: 1.077234]\n",
      "1275 [D loss: 0.721438, acc.: 50.00%] [G loss: 1.047225]\n",
      "1276 [D loss: 0.611694, acc.: 68.75%] [G loss: 0.931293]\n",
      "1277 [D loss: 0.762662, acc.: 50.00%] [G loss: 0.955010]\n",
      "1278 [D loss: 0.768227, acc.: 53.12%] [G loss: 1.099124]\n",
      "1279 [D loss: 0.793177, acc.: 53.12%] [G loss: 0.975464]\n",
      "1280 [D loss: 0.627063, acc.: 68.75%] [G loss: 0.904512]\n",
      "1281 [D loss: 0.665626, acc.: 71.88%] [G loss: 0.948112]\n",
      "1282 [D loss: 0.767090, acc.: 46.88%] [G loss: 0.892689]\n",
      "1283 [D loss: 0.594485, acc.: 71.88%] [G loss: 0.963943]\n",
      "1284 [D loss: 0.642271, acc.: 59.38%] [G loss: 1.010894]\n",
      "1285 [D loss: 0.645797, acc.: 68.75%] [G loss: 0.825325]\n",
      "1286 [D loss: 0.701298, acc.: 56.25%] [G loss: 1.018547]\n",
      "1287 [D loss: 0.872771, acc.: 37.50%] [G loss: 0.971293]\n",
      "1288 [D loss: 0.657012, acc.: 62.50%] [G loss: 0.818631]\n",
      "1289 [D loss: 0.795785, acc.: 43.75%] [G loss: 0.884666]\n",
      "1290 [D loss: 0.758105, acc.: 46.88%] [G loss: 0.903222]\n",
      "1291 [D loss: 0.729082, acc.: 53.12%] [G loss: 0.808815]\n",
      "1292 [D loss: 0.618499, acc.: 56.25%] [G loss: 1.010891]\n",
      "1293 [D loss: 0.655664, acc.: 56.25%] [G loss: 0.972003]\n",
      "1294 [D loss: 0.701197, acc.: 59.38%] [G loss: 0.917125]\n",
      "1295 [D loss: 0.684562, acc.: 65.62%] [G loss: 0.846853]\n",
      "1296 [D loss: 0.747588, acc.: 50.00%] [G loss: 0.834727]\n",
      "1297 [D loss: 0.689931, acc.: 56.25%] [G loss: 0.806758]\n",
      "1298 [D loss: 0.753970, acc.: 56.25%] [G loss: 0.906171]\n",
      "1299 [D loss: 0.688335, acc.: 59.38%] [G loss: 0.966136]\n",
      "1300 [D loss: 0.750589, acc.: 56.25%] [G loss: 1.000821]\n",
      "1301 [D loss: 0.729957, acc.: 59.38%] [G loss: 1.003656]\n",
      "1302 [D loss: 0.629714, acc.: 62.50%] [G loss: 0.937614]\n",
      "1303 [D loss: 0.792477, acc.: 50.00%] [G loss: 1.021232]\n",
      "1304 [D loss: 0.674813, acc.: 56.25%] [G loss: 1.121106]\n",
      "1305 [D loss: 0.828609, acc.: 46.88%] [G loss: 0.961226]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dcgan = DCGAN()\n",
    "    dcgan.train(epochs=4000, batch_size=32, save_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image, HTML\n",
    "def Display_Images(images, header=None, width=\"100%\"): # to match Image syntax\n",
    "    if type(width)==type(1): width = \"{}px\".format(width)\n",
    "    html = [\"<table style='width:{}'><tr>\".format(width)]\n",
    "    if header is not None:\n",
    "        html += [\"<th>{}</th>\".format(h) for h in header] + [\"</tr><tr>\"]\n",
    "\n",
    "    cols=1\n",
    "    for image in images:\n",
    "        print(image)\n",
    "        html.append(\"<td><img src='{}' /></td>\".format(image))\n",
    "        cols+=1\n",
    "        if (cols>3):\n",
    "            html.append(\"</tr><tr>\")\n",
    "            cols=1\n",
    "    html.append(\"</tr></table>\")\n",
    "    display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "file_map=[]\n",
    "for image_path in glob.glob('./' + \"/images/*.png\"):\n",
    "    file_map.append(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".//images/mnist_0.png\n",
      ".//images/mnist_1400.png\n",
      ".//images/mnist_1850.png\n",
      ".//images/mnist_2300.png\n",
      ".//images/mnist_2750.png\n",
      ".//images/mnist_3200.png\n",
      ".//images/mnist_3650.png\n",
      ".//images/mnist_500.png\n",
      ".//images/mnist_950.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style='width:100%'><tr><td><img src='.//images/mnist_0.png' /></td><td><img src='.//images/mnist_1400.png' /></td><td><img src='.//images/mnist_1850.png' /></td></tr><tr><td><img src='.//images/mnist_2300.png' /></td><td><img src='.//images/mnist_2750.png' /></td><td><img src='.//images/mnist_3200.png' /></td></tr><tr><td><img src='.//images/mnist_3650.png' /></td><td><img src='.//images/mnist_500.png' /></td><td><img src='.//images/mnist_950.png' /></td></tr><tr></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indices=[0,10,20,30,40,50,60,70,79]\n",
    "Display_Images([file_map[i] for i in indices],width=\"100%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
